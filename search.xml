<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Centos tar包安装mysql5.7(root)]]></title>
    <url>%2F2019%2F02%2F14%2Flinux-mysql%2F</url>
    <content type="text"><![CDATA[1、由于在线安装受制于网络环境，所以选择tar包编译安装。首先去mysql镜像站下载mysql-5.7.17-linux-glibc2.5-x86_64.tar.gz2、上传到centos3、检查你所用的Linux下有没有安装过mysql，没有卸载干净 rpm -qa|grep -i mysql 如果存在，则需要先卸载掉，不然会出现覆盖错误。 yum remove mysql mysql-server mysql-libs mysql-server;find / -name mysql 将找到的相关东西delete掉；rpm -qa|grep mysql(查询出来的东东yum remove掉) 4、创建mysql的用户组/用户, data目录及其用户目录 userdel mysql # 删除用户groupdel mysql # 删除用户组名mkdir /usr/local/mysql # mysql的默认安装路径，建议不要更换，如果更换后续需要更新配置mkdir /usr/local/mysql/data # 在mysql文件夹下创建文件夹datagroupadd mysql # 创建一个名为mysql的用户组useradd -g mysql -d /usr/local/mysql mysql # 在用户组下创建用户 5、解压缩并转移 tar -xzvf mysql-5.7.17-linux-glibc2.5-x86_64.tar.gz # 解压文件cd mysql-5.7.17-linux-glibc2.5-x86_64 # 进入mv * /usr/local/mysql/ 6、编译安装 cd /usr/local/mysql/./bin/mysqld –user=mysql –basedir=/usr/local/mysql –datadir=/usr/local/mysql/data –initialize 记录上面成功安装后的密码，如上：hIE;k,h8gd#q，后续用到！7、启动mysql服务 ./support-files/mysql.server start 截至目前，证明mysql已运行成功！！！8、配置mysql进入mysql的安装目录支持文件目录 cd /usr/local/mysql//support-files 拷贝配置文件模板为新的mysql配置文件, cp my-default.cnf /etc/my.cnf 设置编码，可按需修改新的配置文件选项， 不修改配置选项， mysql则按默认配置参数运行.如下是我修改配置文件/etc/my.cnf， 设置编码为utf8以防乱码 vim /etc/my.cnf 123456789101112131415161718192021[mysqld] basedir = /usr/local/mysqldatadir = /usr/local/mysql/data character_set_server=utf8init_connect='SET NAMES utf8' [client]default-character-set=utf8 此处需要注释掉##sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES否则后续登陆时会报异常： mysql: [ERROR] unknown variable ‘sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES’ 9、配置mysql服务开机自动启动123456789# cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld # 拷贝启动文件到/etc/init.d/下并重命令为mysqld# chmod 755 /etc/init.d/mysqld # 增加执行权限# chkconfig --list mysqld # 检查自启动项列表中没有mysqld这个，# chkconfig --add mysqld # 如果没有就添加mysqld：# chkconfig mysqld on # 用这个命令设置开机启动： 10、mysql服务的启动/重启/停止12345# service mysqld start # 启动服务# service mysqld restart # 重启服务# service mysqld stop # 停止服务 11、修改mysql用户root的密码1234567891011121314151617181920212223242526272829303132333435mysql -u root -p输入上边自动生成的密码，进入mysql环境Mysql -u root -p-bash : mysql :command not found原因:这是由于系统默认会查找/usr/bin下的命令，如果这个命令不在这个目录下，当然会找不到命令，我们需要做的就是映射一个链接到/usr/bin目录下，相当于建立一个链接文件。首先得知道mysql命令完整路径，比如我的Linux的mysql的路径是：/usr/local/mysql/bin/mysql，我们则可以这样执行命令： /usr/local/mysql/bin/mysql -uroot这样执行命令，或者创建一个软链接 ln -s /usr/local/mysql/bin/mysql /usr/bin/mysql再次尝试就可以进入了mysql&gt; SET PASSWORD = PASSWORD('123456'); # PASSWORD()里面的123456 是设置的新密码，可以设置成你的密码Query OK, 0 rows affected, 1 warning (0.00 sec)Rows matched: 2 Changed: 0 Warnings: 1mysql&gt; ALTER USER 'root'@'localhost' PASSWORD EXPIRE NEVER;Query OK, 0 rows affected, 1 warning (0.00 sec)Rows matched: 2 Changed: 0 Warnings: 1mysql&gt; flush privileges;Query OK, 0 rows affected, 1 warning (0.00 sec)Rows matched: 2 Changed: 0 Warnings: 1 12、更改一些编码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167mysql&gt; use mysqlReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -A Database changedmysql&gt; \s--------------mysql Ver 14.14 Distrib 5.7.17, for linux-glibc2.5 (x86_64) using EditLine wrapper Connection id: 5Current database: mysqlCurrent user: root@localhostSSL: Not in useCurrent pager: stdoutUsing outfile: ''Using delimiter: ;Server version: 5.7.17Protocol version: 10Connection: Localhost via UNIX socketServer characterset: utf8Db characterset: latin1Client characterset: utf8Conn. characterset: utf8UNIX socket: /tmp/mysql.sockUptime: 5 min 8 sec Threads: 1 Questions: 44 Slow queries: 0 Opens: 136 Flush tables: 1 Open tables: 129 Queries per second avg: 0.142-------------- mysql&gt; show variables like 'character%';+--------------------------+----------------------------------+| Variable_name | Value |+--------------------------+----------------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | latin1 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/local/mysql/share/charsets/ |+--------------------------+----------------------------------+8 rows in set (0.01 sec) mysql&gt; SET character_set_database = utf8;Query OK, 0 rows affected, 1 warning (0.00 sec) mysql&gt; show variables like 'character%';+--------------------------+----------------------------------+| Variable_name | Value |+--------------------------+----------------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/local/mysql/share/charsets/ |+--------------------------+----------------------------------+8 rows in set (0.00 sec) mysql&gt; \s--------------mysql Ver 14.14 Distrib 5.7.17, for linux-glibc2.5 (x86_64) using EditLine wrapper Connection id: 5Current database: mysqlCurrent user: root@localhostSSL: Not in useCurrent pager: stdoutUsing outfile: ''Using delimiter: ;Server version: 5.7.17Protocol version: 10Connection: Localhost via UNIX socketServer characterset: utf8Db characterset: utf8Client characterset: utf8Conn. characterset: utf8UNIX socket: /tmp/mysql.sockUptime: 6 min 20 sec Threads: 1 Questions: 50 Slow queries: 0 Opens: 137 Flush tables: 1 Open tables: 130 Queries per second avg: 0.131 13、mysql远程授权12mysql&gt; grant all privileges on *.* to 'root'@'%' identified by '123456';Query OK, 0 rows affected, 1 warning (0.00 sec) 14、验证，可能需要关闭防火墙 service iptables stop 原文：https://blog.csdn.net/weixin_38281964/article/details/82016431]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>mysql</tag>
        <tag>虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机连通外网配置]]></title>
    <url>%2F2019%2F02%2F13%2FcentOS-network%2F</url>
    <content type="text"><![CDATA[修改网络设置在设置中将网络连接方式改为桥接网络，界面名称改为网络适配器提供无限局域网连接，改好后重启 修改ifcfg-eth0配置使用root用户修改配置vi /etc/sysconfig/network-scripts/ifcfg-eth0，将ONBOOT设为yes、NM_CONTROLLED设为no，如下：1234567DEVICE=eth0HWADDR=08:00:27:75:CC:1ETYPE=EthernetUUID=c0f3dc14-bce4-4afc-b613-29259b84f202ONBOOT=yesNM_CONTROLLED=noBOOTPROTO=dhcp 重启并验证配置更改后重启系统reboot或者重启network 服务service network restart，重启后ping www.baidu.com检查是否通外网 如何在virtualbox安装虚拟机请见博客virtualbox虚拟机安装Centos 6]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>virtualbox</tag>
        <tag>centOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DES前端js加密后端java解密]]></title>
    <url>%2F2018%2F11%2F01%2FDES-encry%2F</url>
    <content type="text"><![CDATA[CryptoJS-v3.1.2下载密码：25rr直接上代码 前端需要引入的js注意js引入顺序，先tripledes.js再mode-ecb.js123&lt;script type="text/javascript" src="js/jquery.min.js" &gt;&lt;/script&gt; &lt;script type="text/javascript" src="js/tripledes.js" &gt;&lt;/script&gt; &lt;script type="text/javascript" src="js/mode-ecb.js" &gt;&lt;/script&gt; js做DES加密123456789// DES加密function encryptByDES(message, key) &#123; var keyHex = CryptoJS.enc.Utf8.parse(key); var encrypted = CryptoJS.DES.encrypt(message, keyHex, &#123; mode: CryptoJS.mode.ECB, padding: CryptoJS.pad.Pkcs7 &#125;); return encrypted.toString();&#125; 我们对helloworld进行DES加密，key设置为12345678加密后的结果为 ovATL3QOQmKh0WiTqhkSbg== 后台采用java版本的DES解密123456789public void testDes()&#123; try &#123; String test = "ovATL3QOQmKh0WiTqhkSbg=="; String DESkey = "12345678"; System.out.println(DESUtil.decryption(test,DESkey)); &#125; catch (Exception e) &#123; e.getMessage(); &#125;&#125; java版的DES工具类DESUtil.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203import java.security.InvalidKeyException;import java.security.NoSuchAlgorithmException;import java.security.spec.InvalidKeySpecException;import javax.crypto.BadPaddingException;import javax.crypto.Cipher;import javax.crypto.IllegalBlockSizeException;import javax.crypto.NoSuchPaddingException;import javax.crypto.SecretKey;import javax.crypto.SecretKeyFactory;import javax.crypto.spec.DESKeySpec;/** * DES加解密工具类 * * @author 程高伟 * * @date 2016年6月15日 上午10:02:50 */public class DESUtil &#123; private static final String DES_ALGORITHM = "DES"; /** * DES加密 * * @param plainData 原始字符串 * @param secretKey 加密密钥 * @return 加密后的字符串 * @throws Exception */ public static String encryption(String plainData, String secretKey) throws Exception &#123; Cipher cipher = null; try &#123; cipher = Cipher.getInstance(DES_ALGORITHM); cipher.init(Cipher.ENCRYPT_MODE, generateKey(secretKey)); &#125; catch (NoSuchAlgorithmException e) &#123; e.printStackTrace(); &#125; catch (NoSuchPaddingException e) &#123; e.printStackTrace(); &#125; catch (InvalidKeyException e) &#123; &#125; try &#123; // 为了防止解密时报javax.crypto.IllegalBlockSizeException: Input length must // be multiple of 8 when decrypting with padded cipher异常， // 不能把加密后的字节数组直接转换成字符串 byte[] buf = cipher.doFinal(plainData.getBytes()); return Base64Utils.encode(buf); &#125; catch (IllegalBlockSizeException e) &#123; e.printStackTrace(); throw new Exception("IllegalBlockSizeException", e); &#125; catch (BadPaddingException e) &#123; e.printStackTrace(); throw new Exception("BadPaddingException", e); &#125; &#125; /** * DES解密 * @param secretData 密码字符串 * @param secretKey 解密密钥 * @return 原始字符串 * @throws Exception */ public static String decryption(String secretData, String secretKey) throws Exception &#123; Cipher cipher = null; try &#123; cipher = Cipher.getInstance(DES_ALGORITHM); cipher.init(Cipher.DECRYPT_MODE, generateKey(secretKey)); &#125; catch (NoSuchAlgorithmException e) &#123; e.printStackTrace(); throw new Exception("NoSuchAlgorithmException", e); &#125; catch (NoSuchPaddingException e) &#123; e.printStackTrace(); throw new Exception("NoSuchPaddingException", e); &#125; catch (InvalidKeyException e) &#123; e.printStackTrace(); throw new Exception("InvalidKeyException", e); &#125; try &#123; byte[] buf = cipher.doFinal(Base64Utils.decode(secretData.toCharArray())); return new String(buf); &#125; catch (IllegalBlockSizeException e) &#123; e.printStackTrace(); throw new Exception("IllegalBlockSizeException", e); &#125; catch (BadPaddingException e) &#123; e.printStackTrace(); throw new Exception("BadPaddingException", e); &#125; &#125; /** * 获得秘密密钥 * * @param secretKey * @return * @throws NoSuchAlgorithmException * @throws InvalidKeySpecException * @throws InvalidKeyException */ private static SecretKey generateKey(String secretKey) throws NoSuchAlgorithmException, InvalidKeySpecException, InvalidKeyException &#123; SecretKeyFactory keyFactory = SecretKeyFactory.getInstance(DES_ALGORITHM); DESKeySpec keySpec = new DESKeySpec(secretKey.getBytes()); keyFactory.generateSecret(keySpec); return keyFactory.generateSecret(keySpec); &#125; static private class Base64Utils &#123; static private char[] alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=" .toCharArray(); static private byte[] codes = new byte[256]; static &#123; for (int i = 0; i &lt; 256; i++) codes[i] = -1; for (int i = 'A'; i &lt;= 'Z'; i++) codes[i] = (byte) (i - 'A'); for (int i = 'a'; i &lt;= 'z'; i++) codes[i] = (byte) (26 + i - 'a'); for (int i = '0'; i &lt;= '9'; i++) codes[i] = (byte) (52 + i - '0'); codes['+'] = 62; codes['/'] = 63; &#125; /** * 将原始数据编码为base64编码 */ static private String encode(byte[] data) &#123; char[] out = new char[((data.length + 2) / 3) * 4]; for (int i = 0, index = 0; i &lt; data.length; i += 3, index += 4) &#123; boolean quad = false; boolean trip = false; int val = (0xFF &amp; (int) data[i]); val &lt;&lt;= 8; if ((i + 1) &lt; data.length) &#123; val |= (0xFF &amp; (int) data[i + 1]); trip = true; &#125; val &lt;&lt;= 8; if ((i + 2) &lt; data.length) &#123; val |= (0xFF &amp; (int) data[i + 2]); quad = true; &#125; out[index + 3] = alphabet[(quad ? (val &amp; 0x3F) : 64)]; val &gt;&gt;= 6; out[index + 2] = alphabet[(trip ? (val &amp; 0x3F) : 64)]; val &gt;&gt;= 6; out[index + 1] = alphabet[val &amp; 0x3F]; val &gt;&gt;= 6; out[index + 0] = alphabet[val &amp; 0x3F]; &#125; return new String(out); &#125; /** * 将base64编码的数据解码成原始数据 */ static private byte[] decode(char[] data) &#123; int len = ((data.length + 3) / 4) * 3; if (data.length &gt; 0 &amp;&amp; data[data.length - 1] == '=') --len; if (data.length &gt; 1 &amp;&amp; data[data.length - 2] == '=') --len; byte[] out = new byte[len]; int shift = 0; int accum = 0; int index = 0; for (int ix = 0; ix &lt; data.length; ix++) &#123; int value = codes[data[ix] &amp; 0xFF]; if (value &gt;= 0) &#123; accum &lt;&lt;= 6; shift += 6; accum |= value; if (shift &gt;= 8) &#123; shift -= 8; out[index++] = (byte) ((accum &gt;&gt; shift) &amp; 0xff); &#125; &#125; &#125; if (index != out.length) throw new Error("miscalculated data length!"); return out; &#125; &#125;&#125;]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>js</tag>
        <tag>des</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitlab上合并代码]]></title>
    <url>%2F2018%2F11%2F01%2Fgitlab-merge%2F</url>
    <content type="text"><![CDATA[使用gitlab管理代码，提交代码至dev后要合并到merge上，只需在当前代码线目录下进行如下操作：1、点击右上角+号，选择New merge request，如图2、选择要合并的代码分支和目标代码分支3、Submit merge request4、最后merge]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js java 前后端md5加密]]></title>
    <url>%2F2018%2F10%2F30%2FMd5-jsjava%2F</url>
    <content type="text"><![CDATA[当做用户登陆、注册的用户名与密码加密时，可以在前端用js定义md5函数对值做哈希，如此用户提交表单后，在网络中路由间传递的便是简单加密后的信息，而非明文信息，等表单提交到后台后，可用md5再次对其进行加密，再行存储。还有很多更稳妥有效的方法，有待学习，先将对md5的初步学习结果记录下来，以备后用。1.前端 md5.js123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183function md5(string)&#123; function md5_RotateLeft(lValue, iShiftBits) &#123; return (lValue&lt;&lt;iShiftBits) | (lValue&gt;&gt;&gt;(32-iShiftBits)); &#125; function md5_AddUnsigned(lX,lY)&#123; var lX4,lY4,lX8,lY8,lResult; lX8 = (lX &amp; 0x80000000); lY8 = (lY &amp; 0x80000000); lX4 = (lX &amp; 0x40000000); lY4 = (lY &amp; 0x40000000); lResult = (lX &amp; 0x3FFFFFFF)+(lY &amp; 0x3FFFFFFF); if (lX4 &amp; lY4) &#123; return (lResult ^ 0x80000000 ^ lX8 ^ lY8); &#125; if (lX4 | lY4) &#123; if (lResult &amp; 0x40000000) &#123; return (lResult ^ 0xC0000000 ^ lX8 ^ lY8); &#125; else &#123; return (lResult ^ 0x40000000 ^ lX8 ^ lY8); &#125; &#125; else &#123; return (lResult ^ lX8 ^ lY8); &#125; &#125; function md5_F(x,y,z)&#123; return (x &amp; y) | ((~x) &amp; z); &#125; function md5_G(x,y,z)&#123; return (x &amp; z) | (y &amp; (~z)); &#125; function md5_H(x,y,z)&#123; return (x ^ y ^ z); &#125; function md5_I(x,y,z)&#123; return (y ^ (x | (~z))); &#125; function md5_FF(a,b,c,d,x,s,ac)&#123; a = md5_AddUnsigned(a, md5_AddUnsigned(md5_AddUnsigned(md5_F(b, c, d), x), ac)); return md5_AddUnsigned(md5_RotateLeft(a, s), b); &#125;; function md5_GG(a,b,c,d,x,s,ac)&#123; a = md5_AddUnsigned(a, md5_AddUnsigned(md5_AddUnsigned(md5_G(b, c, d), x), ac)); return md5_AddUnsigned(md5_RotateLeft(a, s), b); &#125;; function md5_HH(a,b,c,d,x,s,ac)&#123; a = md5_AddUnsigned(a, md5_AddUnsigned(md5_AddUnsigned(md5_H(b, c, d), x), ac)); return md5_AddUnsigned(md5_RotateLeft(a, s), b); &#125;; function md5_II(a,b,c,d,x,s,ac)&#123; a = md5_AddUnsigned(a, md5_AddUnsigned(md5_AddUnsigned(md5_I(b, c, d), x), ac)); return md5_AddUnsigned(md5_RotateLeft(a, s), b); &#125;; function md5_ConvertToWordArray(string) &#123; var lWordCount; var lMessageLength = string.length; var lNumberOfWords_temp1=lMessageLength + 8; var lNumberOfWords_temp2=(lNumberOfWords_temp1-(lNumberOfWords_temp1 % 64))/64; var lNumberOfWords = (lNumberOfWords_temp2+1)*16; var lWordArray=Array(lNumberOfWords-1); var lBytePosition = 0; var lByteCount = 0; while ( lByteCount &lt; lMessageLength ) &#123; lWordCount = (lByteCount-(lByteCount % 4))/4; lBytePosition = (lByteCount % 4)*8; lWordArray[lWordCount] = (lWordArray[lWordCount] | (string.charCodeAt(lByteCount)&lt;&lt;lBytePosition)); lByteCount++; &#125; lWordCount = (lByteCount-(lByteCount % 4))/4; lBytePosition = (lByteCount % 4)*8; lWordArray[lWordCount] = lWordArray[lWordCount] | (0x80&lt;&lt;lBytePosition); lWordArray[lNumberOfWords-2] = lMessageLength&lt;&lt;3; lWordArray[lNumberOfWords-1] = lMessageLength&gt;&gt;&gt;29; return lWordArray; &#125;; function md5_WordToHex(lValue)&#123; var WordToHexValue="",WordToHexValue_temp="",lByte,lCount; for(lCount = 0;lCount&lt;=3;lCount++)&#123; lByte = (lValue&gt;&gt;&gt;(lCount*8)) &amp; 255; WordToHexValue_temp = "0" + lByte.toString(16); WordToHexValue = WordToHexValue + WordToHexValue_temp.substr(WordToHexValue_temp.length-2,2); &#125; return WordToHexValue; &#125;; function md5_Utf8Encode(string)&#123; string = string.replace(/\r\n/g,"\n"); var utftext = ""; for (var n = 0; n &lt; string.length; n++) &#123; var c = string.charCodeAt(n); if (c &lt; 128) &#123; utftext += String.fromCharCode(c); &#125;else if((c &gt; 127) &amp;&amp; (c &lt; 2048)) &#123; utftext += String.fromCharCode((c &gt;&gt; 6) | 192); utftext += String.fromCharCode((c &amp; 63) | 128); &#125; else &#123; utftext += String.fromCharCode((c &gt;&gt; 12) | 224); utftext += String.fromCharCode(((c &gt;&gt; 6) &amp; 63) | 128); utftext += String.fromCharCode((c &amp; 63) | 128); &#125; &#125; return utftext; &#125;; var x=Array(); var k,AA,BB,CC,DD,a,b,c,d; var S11=7, S12=12, S13=17, S14=22; var S21=5, S22=9 , S23=14, S24=20; var S31=4, S32=11, S33=16, S34=23; var S41=6, S42=10, S43=15, S44=21; string = md5_Utf8Encode(string); x = md5_ConvertToWordArray(string); a = 0x67452301; b = 0xEFCDAB89; c = 0x98BADCFE; d = 0x10325476; for (k=0;k&lt;x.length;k+=16) &#123; AA=a; BB=b; CC=c; DD=d; a=md5_FF(a,b,c,d,x[k+0], S11,0xD76AA478); d=md5_FF(d,a,b,c,x[k+1], S12,0xE8C7B756); c=md5_FF(c,d,a,b,x[k+2], S13,0x242070DB); b=md5_FF(b,c,d,a,x[k+3], S14,0xC1BDCEEE); a=md5_FF(a,b,c,d,x[k+4], S11,0xF57C0FAF); d=md5_FF(d,a,b,c,x[k+5], S12,0x4787C62A); c=md5_FF(c,d,a,b,x[k+6], S13,0xA8304613); b=md5_FF(b,c,d,a,x[k+7], S14,0xFD469501); a=md5_FF(a,b,c,d,x[k+8], S11,0x698098D8); d=md5_FF(d,a,b,c,x[k+9], S12,0x8B44F7AF); c=md5_FF(c,d,a,b,x[k+10],S13,0xFFFF5BB1); b=md5_FF(b,c,d,a,x[k+11],S14,0x895CD7BE); a=md5_FF(a,b,c,d,x[k+12],S11,0x6B901122); d=md5_FF(d,a,b,c,x[k+13],S12,0xFD987193); c=md5_FF(c,d,a,b,x[k+14],S13,0xA679438E); b=md5_FF(b,c,d,a,x[k+15],S14,0x49B40821); a=md5_GG(a,b,c,d,x[k+1], S21,0xF61E2562); d=md5_GG(d,a,b,c,x[k+6], S22,0xC040B340); c=md5_GG(c,d,a,b,x[k+11],S23,0x265E5A51); b=md5_GG(b,c,d,a,x[k+0], S24,0xE9B6C7AA); a=md5_GG(a,b,c,d,x[k+5], S21,0xD62F105D); d=md5_GG(d,a,b,c,x[k+10],S22,0x2441453); c=md5_GG(c,d,a,b,x[k+15],S23,0xD8A1E681); b=md5_GG(b,c,d,a,x[k+4], S24,0xE7D3FBC8); a=md5_GG(a,b,c,d,x[k+9], S21,0x21E1CDE6); d=md5_GG(d,a,b,c,x[k+14],S22,0xC33707D6); c=md5_GG(c,d,a,b,x[k+3], S23,0xF4D50D87); b=md5_GG(b,c,d,a,x[k+8], S24,0x455A14ED); a=md5_GG(a,b,c,d,x[k+13],S21,0xA9E3E905); d=md5_GG(d,a,b,c,x[k+2], S22,0xFCEFA3F8); c=md5_GG(c,d,a,b,x[k+7], S23,0x676F02D9); b=md5_GG(b,c,d,a,x[k+12],S24,0x8D2A4C8A); a=md5_HH(a,b,c,d,x[k+5], S31,0xFFFA3942); d=md5_HH(d,a,b,c,x[k+8], S32,0x8771F681); c=md5_HH(c,d,a,b,x[k+11],S33,0x6D9D6122); b=md5_HH(b,c,d,a,x[k+14],S34,0xFDE5380C); a=md5_HH(a,b,c,d,x[k+1], S31,0xA4BEEA44); d=md5_HH(d,a,b,c,x[k+4], S32,0x4BDECFA9); c=md5_HH(c,d,a,b,x[k+7], S33,0xF6BB4B60); b=md5_HH(b,c,d,a,x[k+10],S34,0xBEBFBC70); a=md5_HH(a,b,c,d,x[k+13],S31,0x289B7EC6); d=md5_HH(d,a,b,c,x[k+0], S32,0xEAA127FA); c=md5_HH(c,d,a,b,x[k+3], S33,0xD4EF3085); b=md5_HH(b,c,d,a,x[k+6], S34,0x4881D05); a=md5_HH(a,b,c,d,x[k+9], S31,0xD9D4D039); d=md5_HH(d,a,b,c,x[k+12],S32,0xE6DB99E5); c=md5_HH(c,d,a,b,x[k+15],S33,0x1FA27CF8); b=md5_HH(b,c,d,a,x[k+2], S34,0xC4AC5665); a=md5_II(a,b,c,d,x[k+0], S41,0xF4292244); d=md5_II(d,a,b,c,x[k+7], S42,0x432AFF97); c=md5_II(c,d,a,b,x[k+14],S43,0xAB9423A7); b=md5_II(b,c,d,a,x[k+5], S44,0xFC93A039); a=md5_II(a,b,c,d,x[k+12],S41,0x655B59C3); d=md5_II(d,a,b,c,x[k+3], S42,0x8F0CCC92); c=md5_II(c,d,a,b,x[k+10],S43,0xFFEFF47D); b=md5_II(b,c,d,a,x[k+1], S44,0x85845DD1); a=md5_II(a,b,c,d,x[k+8], S41,0x6FA87E4F); d=md5_II(d,a,b,c,x[k+15],S42,0xFE2CE6E0); c=md5_II(c,d,a,b,x[k+6], S43,0xA3014314); b=md5_II(b,c,d,a,x[k+13],S44,0x4E0811A1); a=md5_II(a,b,c,d,x[k+4], S41,0xF7537E82); d=md5_II(d,a,b,c,x[k+11],S42,0xBD3AF235); c=md5_II(c,d,a,b,x[k+2], S43,0x2AD7D2BB); b=md5_II(b,c,d,a,x[k+9], S44,0xEB86D391); a=md5_AddUnsigned(a,AA); b=md5_AddUnsigned(b,BB); c=md5_AddUnsigned(c,CC); d=md5_AddUnsigned(d,DD); &#125; return (md5_WordToHex(a)+md5_WordToHex(b)+md5_WordToHex(c)+md5_WordToHex(d)).toLowerCase(); &#125; 在login.jsp页面里引用示例：1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;%@ page language="java" import="java.util.*" pageEncoding="UTF-8"%&gt;&lt;%@ taglib uri="/struts-tags" prefix="s" %&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;登陆&lt;/title&gt; &lt;link type="text/css" rel="stylesheet" href="$&#123;pageContext.request.contextPath&#125;/style/reset.css"&gt; &lt;link type="text/css" rel="stylesheet" href="$&#123;pageContext.request.contextPath&#125;/style/main.css"&gt; &lt;script type="text/javascript" src="$&#123;pageContext.request.contextPath&#125;/js/md5.js"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body align = "center"&gt; &lt;div class="headerBar"&gt; &lt;div class="logoBar login_logo"&gt; &lt;div class="comWidth"&gt; &lt;h3 class="welcome_title" &gt;欢迎登陆&lt;/h3&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;s:form action="user_login" method="post" namespace="/" &gt;&lt;div class="loginBox"&gt; &lt;h3&gt;&lt;s:actionerror/&gt;&lt;/h3&gt; &lt;div class="login_cont"&gt; &lt;ul class="login"&gt; &lt;li class="l_tit"&gt;用户名&lt;/li&gt; &lt;li class="mb_10"&gt;&lt;input type="text" name="username" id="username" class="login_input user_icon" placeholder="请输入用户名"&gt;&lt;/li&gt; &lt;li class="l_tit"&gt;密码&lt;/li&gt; &lt;li class="mb_10"&gt;&lt;input type="password" name="password" id="password" class="login_input user_icon" placeholder="请输入密码"&gt;&lt;/li&gt; &lt;li&gt;&lt;input type="submit" value="" class="login_btn" onclick="login()"&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;&lt;/s:form&gt; &lt;/body&gt; &lt;strong&gt; &lt;script type="text/javascript"&gt; function login()&#123; document.getElementById("username").value = md5(document.getElementById("username").value); document.getElementById("password").value = md5(document.getElementById("password").value); &#125; &lt;/script&gt; &lt;/strong&gt;&lt;/html&gt; 2.后台Java 使用MD5算法1234567891011121314151617181920212223242526public String md5Encode(String inStr)&#123; MessageDigest md5 = null; try&#123; md5 = MessageDigest.getInstance("MD5"); &#125;catch(Exception e)&#123; System.out.println(e.toString()); e.printStackTrace(); return ""; &#125; byte[] byteArray = null; try &#123; byteArray = inStr.getBytes("UTF-8"); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; byte[] md5Bytes = md5.digest(byteArray); StringBuffer hexValue = new StringBuffer(); for(int i=0;i&lt;md5Bytes.length;i++)&#123; int val = md5Bytes[i] &amp; 0xff; if(val&lt;16)&#123; hexValue.append("0"); &#125; hexValue.append(Integer.toHexString(val)); &#125; return hexValue.toString(); &#125; 如此便可得到32位的加密后的16进制字符串，亲测对中英文数字等前后端计算结果一致，即用js和用Java加密一次获取到的结果一致，以前找到过两个md5.js计算中文的md5值时结果与后台采用Java的结果不一致，但上文中此版可以。还有，为提高安全性，还可为md5加盐值，例子见此贴一种简单的给MD5加盐算法 在自己做demo的时候，将用户的md5摘要做rowkey，用户的密码的md5摘要的第11-15位替换为rowkey对应的11-15位进行存储，之前看到有说可以将用户名和密码相加求MD5后再截取固定长度也不错。 原文：https://blog.csdn.net/arenn/article/details/53538494]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>md5</tag>
        <tag>java</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前后端实现字符串反转]]></title>
    <url>%2F2018%2F10%2F29%2Fstring-reverse%2F</url>
    <content type="text"><![CDATA[因base64加密后可破解，为了提高安全性，在前端将加密后的密码做字符串反转后提交，然后在后端反转回来再解密。例如:“abcdef”，反转后为“fedcba”，在反转回来“abcdef”。 前端js实现字符串反转方法一（使用中）1234567var str = "abcdef"; console.log( str.split("").reverse().join("") )；'); //分解var str="abcdefg";var a=str.split(""); //先拆分成数组var b=str.split("").reverse(); //再反转，但还是数组var c=str.split("").reverse().join("");//最后把数组变成字符串 方法二12345678var str="abcdef" var i=str.length; i=i-1; for (var x = i; x &gt;=0; x--) &#123; document.write(str.charAt(x)); &#125; //这种方法只是调过来输出而已 方法三123456789&lt;script&gt;var str="abcdef"var a=str.split("");var rs = new Array;while(a.length)&#123; rs.push(a.pop());&#125; alert(rs.join(""));&lt;/script&gt; 后端java实现字符串反转方法一（使用中）123public static String reverse(String str)&#123; return new StringBuilder(str).reverse().toString(); &#125; 方法二12345678910public void reverse() &#123; String str = "abcdef"; for (int i = str.length() - 1; i &gt;= 0; i--) &#123; char c = str.charAt(i); System.out.print(c); &#125; &#125; 方法三1234567891011121314151617181920212223242526272829303132public class StringReverse &#123; public void swap(char[] arr, int begin, int end) &#123; while(begin &lt; end) &#123; char temp = arr[begin]; arr[begin] = arr[end]; arr[end] = temp; begin++; end--; &#125; &#125; //I love java public String swapWords(String str) &#123; char[] arr = str.toCharArray(); swap(arr, 0, arr.length - 1); int begin = 0; for (int i = 1; i &lt; arr.length; i++) &#123; if (arr[i] == ' ') &#123; swap(arr, begin, i - 1); begin = i + 1; &#125; &#125; return new String(arr); &#125; public static void main(String[] args) &#123; String str = "I love java"; System.out.println(new StringReverse().swapWords(str)); &#125; &#125; 方法四：利用JDK提供的方法123456789101112131415161718192021222324public class Main &#123; public static void main(String[] args) &#123; Scanner sc = new Scanner(System.in); String str = sc.nextLine(); String[] sArr = str.split(" ");//I love java List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list = Arrays.asList(sArr); // for(int i=0;i&lt;sArr.length;i++)&#123; // list.add(sArr[i]); // &#125; Collections.reverse(list); for(String word:list)&#123; System.out.print(word+" "); &#125; &#125; &#125;]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>js</tag>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[md5sum使用]]></title>
    <url>%2F2018%2F09%2F29%2Flinux-md5sum%2F</url>
    <content type="text"><![CDATA[1、简介MD5算法一般用于检查文件完整性，尤其常用于检测在（网络）文件传输、拷贝、磁盘错误或其他无恶意涉入的情况下文件的正确性。MD5全称报文摘要算法（Message-DigestAlgorithm5）[RFC 1321]，该算法对任意长度的信息进行逐位计算，产生一个二进制长度128位（十六进制长度32位）的校验和（或称“指纹”，“报文摘要”），不同的文件内容生成相同的报文摘要的概率是极其小的。 在Linux或Unix上，md5sum是用来计算和校验文件报文摘要的工具程序。一般来说，安装了Linux后，就会有md5sum工具，可以在命令行终端直接执行。 2、简单使用查看文件的md5值1md5sum filename 3、命令格式12345678910111213md5sum [OPTION]... [FILE]...命令选项-b或 --binary:以二进制模式读入文件；-t或 --text:以文本文件模式读入文件（默认）；-c或 --check:用来从文件中读取md5信息检查文件的一致性；--status:该选项与check一起使用，在check时不输出，根据返回值表示检查结果；-w或 --warn:在check时，检查输入的md5信息有没有非法行，若有则输出相应信息。 4、实例演示1）生成一个文件的md5值，如下所示：12345678910111213141516171819[root@HLZ test]# lltotal 16-rw-r--r--1 root root 240 Sep 4 22:26 aaa_pkg.tgz-rw-r--r--1 root root 11 Sep 4 22:01 aaa.txt-rw-r--r--1 root root 46 Sep 4 22:24 bbb.logdrwxr-xr-x2 root root 4096 Sep 4 22:24 checksum[root@HLZ test]#[root@HLZ test]#md5sum aaa.txt &gt; checksum/aaa.md5 这样，就生成了文件aaa.txt的md5校验文件aaa.md5，打开可以看到如下内容：1234[root@HLZ test]#cat checksum/aaa.md5 d700cf9bb133858df4d3e0486848be78 aaa.txt 2）md5sum支持多个文件输入或通配符123456789101112131415161718192021[root@HLZ test]#md5sum aaa.txt bbb.log aaa_pkg.tgz &gt; checksum/all.md5[root@HLZ test]#cat checksum/all.md5 d700cf9bb133858df4d3e0486848be78 aaa.txtfff62b77988e5b9d83a2872bb2247b21 bbb.log36eb58b2563f91baf61531aef126eea6 aaa_pkg.tgz[root@HLZ test]#md5sum aaa* &gt; checksum/aaa_all.md5[root@HLZ test]#cat checksum/aaa_all.md5 36eb58b2563f91baf61531aef126eea6 aaa_pkg.tgzd700cf9bb133858df4d3e0486848be78 aaa.txt 3）文件内容校验12345678910111213141516171819202122232425262728293031[root@HLZ test]#md5sum -c ./checksum/aaa.md5aaa.txt:OK[root@HLZ test]#md5sum -c ./checksum/all.md5aaa.txt:OKbbb.log:OKaaa_pkg.tgz:OK[root@HLZ test]#vi bbb.log ---修改bbb.log文件内容，添加7个0abceefghijklmnopqr0000000stuvwxyz1234567890987654321[root@HLZ test]#md5sum -c ./checksum/all.md5 aaa.txt:OKbbb.log: FAILEDaaa_pkg.tgz:OKmd5sum: WARNING: 1 of 3 computed checksums did NOT match[root@HLZ test]# 4）文件路径的考虑（$PWD即当前路径/home/hanlzh/test/）123456789101112131415161718192021222324[root@HLZ test]#pwd/home/hanlzh/test[root@HLZ test]#md5sum $PWD/aaa.txt &gt; ./checksum/aaa.md5 [root@HLZ test]# cat checksum/aaa.md5d700cf9bb133858df4d3e0486848be78 /home/hanlzh/test/aaa.txt[root@HLZ test]#md5sum -c ./checksum/aaa.md5/home/hanlzh/test/aaa.txt:OK[root@HLZ test]#cd ..[root@HLZ hanlzh]#md5sum -c ./test/checksum/aaa.md5/home/hanlzh/test/aaa.txt:OK 4）文件缺失或不存在情况12345678910111213[root@HLZ test]#rm -rf aaa_pkg.tgz [root@HLZ test]#md5sum -c ./checksum/aaa_all.md5 md5sum: aaa_pkg.tgz: No such file or directoryaaa_pkg.tgz: FAILED open or readaaa.txt:OKmd5sum: WARNING: 1 of 2 listed files could not be read 5、总结特殊说明 1）md5sum是校验文件内容，与文件名是否相同无关； 2）md5sum是逐位校验，所以文件越大，校验时间越长。 md5校验，可能极小概率出现不同的文件生成相同的校验和，比md5更安全的校验算法还有SHA*系列，如sha1sum/sha224sum/sha256sum/sha384sum/sha512sum等等，基本用法与md5sum命令类似，详情可通过man sha1sum查询。 以下仅简单列举一例，不再赘述。1234567891011121314[root@HLZ test]#sha512sum aaa.txt &gt; aaa.sha512[root@HLZ test]#cat aaa.sha512 8edb9790359ef641112fa85eea5c8f09b7330564a58cddf39aef66006f32454a8879b1e63d9f667ecf86264d4d5b6a602a4c94c79d962ec755345a3837217f89 aaa.txt[root@HLZ test]#[root@HLZ test]#sha512sum -c ./aaa.sha512aaa.txt:OK 原文：https://blog.csdn.net/hanlizhong85/article/details/77844635]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>md5sum使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Powershell中实现连续命令]]></title>
    <url>%2F2018%2F09%2F21%2Fwindows-powershell%2F</url>
    <content type="text"><![CDATA[cmd中有“;”和“&amp;&amp;”来实现命令的连续执行，在Powershell中怎么来实现呢？powershell用”;”实现，亲测有效示例：hexo clean ; hexo g ;hexo s]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>windows</tag>
        <tag>windows10</tag>
        <tag>powershell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异常 The processing instruction target matching [xX][mM][lL] is not allowed]]></title>
    <url>%2F2018%2F09%2F21%2FError-xml-01%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021[Fatal Error] hive-site.xml:3:6: The processing instruction target matching "[xX][mM][lL]" is not allowed.18/09/21 10:26:53 FATAL conf.Configuration: error parsing conf file:/home/hadoop/apache-hive-1.2.0-bin/conf/hive-site.xmlorg.xml.sax.SAXParseException; systemId: file:/home/hadoop/apache-hive-1.2.0-bin/conf/hive-site.xml; lineNumber: 3; columnNumber: 6; The processing instruction target matching "[xX][mM][lL]" is not allowed. at org.apache.xerces.parsers.DOMParser.parse(Unknown Source) at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source) at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150) at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2352) at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2340) at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2408) at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2374) at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2281) at org.apache.hadoop.conf.Configuration.get(Configuration.java:1108) at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:2605) ▽&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; 这个异常解释为：xml文件不能被解析，一般出现这样的问题在于xml格式上，并且问题多出现在xml文件的头部，这是因为xml文件头部有两行空行，第三行才开始写总结：&lt;?xml version=”1.0” encoding=”UTF-8”?&gt;前面不要有任何其他字符，如空格、回车、换行这些否则就会出现上面的异常。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>error</tag>
        <tag>xml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-JAVA JPS 命令详解]]></title>
    <url>%2F2018%2F09%2F20%2Flinux-jps%2F</url>
    <content type="text"><![CDATA[JAVA JPS 命令详解 JPS 名称: jps - Java Virtual Machine Process Status Tool命令用法: jps [options] [hostid] options:命令选项，用来对输出格式进行控制 hostid:指定特定主机，可以是ip地址和域名, 也可以指定具体协议，端口。 [protocol:][[//]hostname][:port][/servername] 功能描述: jps是用于查看有权访问的hotspot虚拟机的进程. 当未指定hostid时，默认查看本机jvm进程，否者查看指定的hostid机器上的jvm进程，此时hostid所指机器必须开启jstatd服务。 jps可以列出jvm进程lvmid，主类类名，main函数参数, jvm参数，jar名称等信息。命令选项及功能：没添加option的时候，默认列出VM标示符号和简单的class或jar名称.如下:1234567[hadoop@sz-pg-smce-cce-014:/home/hadoop]$ jps21794 Jps24243 start.jar21316 start.jar26203 crowd-0.0.1.jar21551 Application[hadoop@sz-pg-smce-cce-014:/home/hadoop]$ -q :仅仅显示VM 标示，不显示jar,class, main参数等信息.1234567[hadoop@sz-pg-smce-cce-014:/home/hadoop]$ jps -q2424321316218502620321551[hadoop@sz-pg-smce-cce-014:/home/hadoop]$ -m:输出主函数传入的参数. 下的hello 就是在执行程序时从命令行输入的参数1234567[hadoop@sz-pg-smce-cce-014:/home/hadoop]$ jps -m24243 start.jar jetty.state=/home/hadoop/jetty-9.2.10-sso-9293/jetty.state jetty-logging.xml jetty-started.xml21316 start.jar jetty.state=/home/hadoop/jetty-9.2.10-um-9291/jetty.state jetty-logging.xml jetty-started.xml21894 Jps -m26203 crowd-0.0.1.jar21551 Application[hadoop@sz-pg-smce-cce-014:/home/hadoop]$ -l: 输出应用程序主类完整package名称或jar完整名称.1234567[hadoop@sz-pg-smce-cce-014:/home/hadoop]$ jps -l21936 sun.tools.jps.Jps24243 /home/hadoop/jetty-9.2.10-sso-9293/start.jar21316 /home/hadoop/jetty-9.2.10-um-9291/start.jar26203 /home/hadoop/crowd/crowd-0.0.1.jar21551 td.enterprise.portal.Application[hadoop@sz-pg-smce-cce-014:/home/hadoop]$ -v: 列出jvm参数, -Xms20m -Xmx50m是启动程序指定的jvm参数1234567[hadoop@sz-pg-smce-cce-014:/home/hadoop]$ jps -v24243 start.jar -Djetty.logs=/home/hadoop/jetty-9.2.10-sso-9293/logs -Djetty.home=/home/hadoop/jetty-9.2.10-sso-9293 -Djetty.base=/home/hadoop/jetty-9.2.10-sso-9293 -Djava.io.tmpdir=/tmp -Dconfig.server.url=http://172.23.6.139:9098/configservice -Dfile.encoding=UTF-8 -Xms512m -Xmx2048m -XX:PermSize=128m -XX:MaxPermSize=256m -Dconfig.local.path=/home/hadoop/jetty-9.2.10-sso-929321316 start.jar -Djetty.logs=/home/hadoop/jetty-9.2.10-um-9291/logs -Djetty.home=/home/hadoop/jetty-9.2.10-um-9291 -Djetty.base=/home/hadoop/jetty-9.2.10-um-9291 -Djava.io.tmpdir=/tmp -Dconfig.server.url=http://172.23.6.139:9098/configservice -Dfile.encoding=UTF-8 -Xms512m -Xmx2048m -XX:PermSize=128m -XX:MaxPermSize=256m -Dconfig.local.path=/home/hadoop/jetty-9.2.10-um-929121978 Jps -Dapplication.home=/home/hadoop/jdk1.8.0_131 -Xms8m26203 crowd-0.0.1.jar -Dspring.profiles.active=prod -Dlogging.path=/home/hadoop/logs21551 Application -Xms1536m -Xmx1536m -Djava.net.preferIPv4Stack=true -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/hadoop/dump/portal[hadoop@sz-pg-smce-cce-014:/home/hadoop]$ -V: 输出通过.hotsportrc或-XX:Flags=指定的jvm参数-Joption:传递参数到javac 调用的java lancher. 扩展jps命令也可使用搜索命令，提高检索效率，如：123[hadoop@sz-pg-smce-cce-014:/home/hadoop]$ jps -l|grep um21316 /home/hadoop/jetty-9.2.10-um-9291/start.jar[hadoop@sz-pg-smce-cce-014:/home/hadoop]$]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>linux</tag>
        <tag>jps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux配置nginx强制使用https访问(http跳转到https)]]></title>
    <url>%2F2018%2F09%2F19%2Flinux-https-rewrite%2F</url>
    <content type="text"><![CDATA[思路最容易想到的方法，将所有的http请求通过rewrite重写到https上即可配置123456server &#123; listen 80; server_name bestsmartfoot.top; rewrite ^(.*)$ https://$host$1 permanent; &#125; 效果搭建此虚拟主机完成后，就可以将http://bestsmartfoot.top、bestsmartfoot.top、www.bestsmartfoot.top的请求全部重写到https://www.bestsmartfoot.top/上了]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nginx</tag>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ETL的经验总结]]></title>
    <url>%2F2018%2F09%2F18%2FETL-summary%2F</url>
    <content type="text"><![CDATA[ETL的考虑&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;做数据仓库系统，ETL是关键的一环。说大了，ETL是数据整合解决方案，说小了，就是倒数据的工具。回忆一下工作这么些年来，处理数据迁移、转换的工作倒还真的不少。但是那些工作基本上是一次性工作或者很小数据量，使用access、DTS或是自己编个小程序搞定。可是在数据仓库系统中，ETL上升到了一定的理论高度，和原来小打小闹的工具使用不同了。究竟什么不同，从名字上就可以看到，人家已经将倒数据的过程分成3个步骤，E、T、L分别代表抽取、转换和装载。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实ETL过程就是数据流动的过程，从不同的数据源流向不同的目标数据。但在数据仓库中，ETL有几个特点，一是数据同步，它不是一次性倒完数据就拉到，它是经常性的活动，按照固定周期运行的，甚至现在还有人提出了实时ETL的概念。二是数据量，一般都是巨大的，值得你将数据流动的过程拆分成E、T和L。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在有很多成熟的工具提供ETL功能，例如datastage、powermart等，且不说他们的好坏。从应用角度来说，ETL的过程其实不是非常复杂，这些工具给数据仓库工程带来和很大的便利性，特别是开发的便利和维护的便利。但另一方面，开发人员容易迷失在这些工具中。举个例子，VB是一种非常简单的语言并且也是非常易用的编程工具，上手特别快，但是真正VB的高手有多少？微软设计的产品通常有个原则是”将使用者当作傻瓜”，在这个原则下，微软的东西确实非常好用，但是对于开发者，如果你自己也将自己当作傻瓜，那就真的傻了。ETL工具也是一样，这些工具为我们提供图形化界面，让我们将主要的精力放在规则上，以期提高开发效率。从使用效果来说，确实使用这些工具能够非常快速地构建一个job来处理某个数据，不过从整体来看，并不见得他的整体效率会高多少。问题主要不是出在工具上，而是在设计、开发人员上。他们迷失在工具中，没有去探求ETL的本质。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以说这些工具应用了这么长时间，在这么多项目、环境中应用，它必然有它成功之处，它必定体现了ETL的本质。如果我们不透过表面这些工具的简单使用去看它背后蕴涵的思想，最终我们作出来的东西也就是一个个独立的job，将他们整合起来仍然有巨大的工作量。大家都知道“理论与实践相结合”，如果在一个领域有所超越，必须要在理论水平上达到一定的高度 探求ETL本质之一&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ETL的过程就是数据流动的过程，从不同异构数据源流向统一的目标数据。其间，数据的抽取、清洗、转换和装载形成串行或并行的过程。ETL的核心还是在于T这个过程，也就是转换，而抽取和装载一般可以作为转换的输入和输出，或者，它们作为一个单独的部件，其复杂度没有转换部件高。和OLTP系统中不同，那里充满这单条记录的insert、update和select等操作，ETL过程一般都是批量操作，例如它的装载多采用批量装载工具，一般都是DBMS系统自身附带的工具，例如Oracle SQLLoader和DB2的autoloader等。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ETL本身有一些特点，在一些工具中都有体现，下面以datastage和powermart举例来说。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1、静态的ETL单元和动态的ETL单元实例。一次转换指明了某种格式的数据如何格式化成另一种格式的数据，对于数据源的物理形式在设计时可以不用指定，它可以在运行时，当这个ETL单元创建一个实例时才指定。对于静态和动态的ETL单元，Datastage没有严格区分，它的一个Job就是实现这个功能，在早期版本，一个Job同时不能运行两次，所以一个Job相当于一个实例，在后期版本，它支持multiple instances，而且还不是默认选项。Powermart中将这两个概念加以区分，静态的叫做Mapping，动态运行时叫做Session。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2、ETL元数据。元数据是描述数据的数据，他的含义非常广泛，这里仅指ETL的元数据。主要包括每次转换前后的数据结构和转换的规则。ETL元数据还包括形式参数的管理，形式参数的ETL单元定义的参数，相对还有实参，它是运行时指定的参数，实参不在元数据管理范围之内。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3、数据流程的控制。要有可视化的流程编辑工具，提供流程定义和流程监控功能。流程调度的最小单位是ETL单元实例，ETL单元是不能在细分的ETL过程，当然这由开发者来控制，例如可以将抽取、转换放在一个ETL单元中，那样这个抽取和转换只能同时运行，而如果将他们分作两个单元，可以分别运行，这有利于错误恢复操作。当然，ETL单元究竟应该细分到什么程度应该依据具体应用来看，目前还没有找到很好的细分策略。比如，我们可以规定将装载一个表的功能作为一个ETL单元，但是不可否认，这样的ETL单元之间会有很多共同的操作，例如两个单元共用一个Hash表，要将这个Hash表装入内存两次。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4、转换规则的定义方法。提供函数集提供常用规则方法，提供规则定义语言描述规则。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5、对数据的快速索引。一般都是利用Hash技术，将参照关系表提前装入内存，在转换时查找这个hash表。Datastage中有Hash文件技术，Powermart也有类似的Lookup功能。 探求ETL本质之二(分类)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;昨在IT-Director上阅读一篇报告，关于ETL产品分类的。一般来说，我们眼中的ETL工具都是价格昂贵，能够处理海量数据的家伙，但是这是其中的一种。它可以分成4种，针对不同的需求，主要是从转换规则的复杂度和数据量大小来看。它们包括:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1、交互式运行环境。你可以指定数据源、目标数据，指定规则，立马ETL。这种交互式的操作无疑非常方便，但是只能适合小数据量和复杂度不高的ETL过程，因为一旦规则复杂了，可能需要语言级的描述，不能简简单单拖拖拽拽就可以的。还有数据量的问题，这种交互式必然建立在解释型语言基础上，另外他的灵活性必然要牺牲一定的性能为代价。所以如果要处理海量数据的话，每次读取一条记录，每次对规则进行解释执行，每次在写入一条记录，这对性能影响是非常大的。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2、专门编码型的。它提供了一个基于某种语言的程序框架，你可以不必将编程精力放在一些周边的功能上，例如读文件功能、写数据库的功能，而将精力主要放在规则的实现上面。这种近似手工代码的性能肯定是没话说，除非你的编程技巧不过关（这也是不可忽视的因素之一）。对于处理大数据量，处理复杂转换逻辑，这种方式的ETL实现是非常直观的。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3、代码生成器型的。它就像是一个ETL代码生成器，提供简单的图形化界面操作，让你拖拖拽拽将转换规则都设定好，其实他的后台都是生成基于某种语言的程序，要运行这个ETL过程，必须要编译才行。Datastage就是类似这样的产品，设计好的job必须要编译，这避免了每次转换的解释执行，但是不知道它生成的中间语言是什么。以前我设计的ETL工具大挪移其实也是归属于这一类，它提供了界面让用户编写规则，最后生成C++语言，编译后即可运行。这类工具的特点就是要在界面上下狠功夫，必须让用户轻松定义一个ETL过程，提供丰富的插件来完成读、写和转换函数。大挪移在这方面就太弱了，规则必须手写，而且要写成标准c++语法，这未免还是有点难为最终用户了，还不如做成一个专业编码型的产品呢。另外一点，这类工具必须提供面向专家应用的功能，因为它不可能考虑到所有的转换规则和所有的读写，一方面提供插件接口来让第三方编写特定的插件，另一方面还有提供特定语言来实现高级功能。例如Datastage提供一种类Basic的语言，不过他的Job的脚本化实现好像就做的不太好，只能手工绘制job，而不能编程实现Job。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4、最后还有一种类型叫做数据集线器。顾名思义，他就是像Hub一样地工作。将这种类型分出来和上面几种分类在标准上有所差异，上面三种更多指ETL实现的方法，此类主要从数据处理角度。目前有一些产品属于EAI（Enterprise Application Integration），它的数据集成主要是一种准实时性。所以这类产品就像Hub一样，不断接收各种异构数据源来的数据，经过处理，在实施发送到不同的目标数据中去。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;虽然，这些类看似各又千秋，特别在BI项目中，面对海量数据的ETL时，中间两种的选择就开始了，在选择过程中，必须要考虑到开发效率、维护方面、性能、学习曲线、人员技能等各方面因素，当然还有最重要也是最现实的因素就是客户的意象。 探求ETL本质之三(转换)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ETL探求之一中提到，ETL过程最复杂的部分就是T，这个转换过程，T过程究竟有哪些类型呢？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一、宏观输入输出&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从对数据源的整个宏观处理分，看看一个ETL过程的输入输出，可以分成下面几类：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1、大小交。这种处理在数据清洗过程是常见了，例如从数据源到ODS阶段，如果数据仓库采用维度建模，而且维度基本采用代理键的话，必然存在代码到此键值的转换。如果用SQL实现，必然需要将一个大表和一堆小表都Join起来，当然如果使用ETL工具的话，一般都是先将小表读入内存中再处理。这种情况，输出数据的粒度和大表一样。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2、大大交。大表和大表之间关联也是一个重要的课题，当然其中要有一个主表，在逻辑上，应当是主表Left Join辅表。大表之间的关联存在最大的问题就是性能和稳定性，对于海量数据来说，必须有优化的方法来处理他们的关联，另外，对于大数据的处理无疑会占用太多的系统资源，出错的几率非常大，如何做到有效错误恢复也是个问题。对于这种情况，我们建议还是尽量将大表拆分成适度的稍小一点的表，形成大小交的类型。这类情况的输出数据粒度和主表一样。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3、站着进来，躺着出去。事务系统中为了提高系统灵活性和扩展性，很多信息放在代码表中维护，所以它的”事实表”就是一种窄表，而在数据仓库中，通常要进行宽化，从行变成列，所以称这种处理情况叫做”站着进来，躺着出去”。大家对Decode肯定不陌生，这是进行宽表化常见的手段之一。窄表变宽表的过程主要体现在对窄表中那个代码字段的操作。这种情况，窄表是输入，宽表是输出，宽表的粒度必定要比窄表粗一些，就粗在那个代码字段上。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4、聚集。数据仓库中重要的任务就是沉淀数据，聚集是必不可少的操作，它是粗化数据粒度的过程。聚集本身其实很简单，就是类似SQL中Group by的操作，选取特定字段（维度），对度量字段再使用某种聚集函数。但是对于大数据量情况下，聚集算法的优化仍是探究的一个课题。例如是直接使用SQL的Group by，还是先排序，在处理。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;二、微观规则&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从数据的转换的微观细节分，可以分成下面的几个基本类型，当然还有一些复杂的组合情况，例如先运算，在参照转换的规则，这种基于基本类型组合的情况就不在此列了。ETL的规则是依赖目标数据的，目标数据有多少字段，就有多少条规则。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1、直接映射。原来是什么就是什么，原封不动照搬过来，对这样的规则，如果数据源字段和目标字段长度或精度不符，需要特别注意看是否真的可以直接映射还是需要做一些简单运算。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2、字段运算。数据源的一个或多个字段进行数学运算得到的目标字段，这种规则一般对数值型字段而言。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3、参照转换。在转换中通常要用数据源的一个或多个字段作为Key，去一个关联数组中去搜索特定值，而且应该只能得到唯一值。这个关联数组使用Hash算法实现是比较合适也是最常见的，在整个ETL开始之前，它就装入内存，对性能提高的帮助非常大。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4、字符串处理。从数据源某个字符串字段中经常可以获取特定信息，例如身份证号。而且，经常会有数值型值以字符串形式体现。对字符串的操作通常有类型转换、字符串截取等。但是由于字符类型字段的随意性也造成了脏数据的隐患，所以在处理这种规则的时候，一定要加上异常处理。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5、空值判断。对于空值的处理是数据仓库中一个常见问题，是将它作为脏数据还是作为特定一种维成员？这恐怕还要看应用的情况，也是需要进一步探求的。但是无论怎样，对于可能有NULL值的字段，不要采用“直接映射”的规则类型，必须对空值进行判断，目前我们的建议是将它转换成特定的值。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6、日期转换。在数据仓库中日期值一般都会有特定的，不同于日期类型值的表示方法，例如使用8位整型20040801表示日期。而在数据源中，这种字段基本都是日期类型的，所以对于这样的规则，需要一些共通函数来处理将日期转换为8位日期值、6位月份值等。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7、日期运算。基于日期，我们通常会计算日差、月差、时长等。一般数据库提供的日期运算函数都是基于日期型的，而在数据仓库中采用特定类型来表示日期的话，必须有一套自己的日期运算函数集。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8、聚集运算。对于事实表中的度量字段，他们通常是通过数据源一个或多个字段运用聚集函数得来的，这些聚集函数为SQL标准中，包括sum,count,avg,min,max。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;9、既定取值。这种规则和以上各种类型规则的差别就在于它不依赖于数据源字段，对目标字段取一个固定的或是依赖系统的值。 探求ETL本质之四(数据质量)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;“不要绝对的数据准确，但要知道为什么不准确。”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是我们在构建BI系统是对数据准确性的要求。确实，对绝对的数据准确谁也没有把握，不仅是系统集成商，包括客户也是无法确定。准确的东西需要一个标准，但首先要保证这个标准是准确的，至少现在还没有这样一个标准。客户会提出一个相对标准，例如将你的OLAP数据结果和报表结果对比。虽然这是一种不太公平的比较，你也只好认了吧。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先在数据源那里，已经很难保证数据质量了，这一点也是事实。在这一层有哪些可能原因导致数据质量问题？可以分为下面几类：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1、数据格式错误。例如缺失数据、数据值超出范围或是数据格式非法等。要知道对于同样处理大数据量的数据源系统，他们通常会舍弃一些数据库自身的检查机制，例如字段约束等。他们尽可能将数据检查在入库前保证，但是这一点是很难确保的。这类情况诸如身份证号码、手机号、非日期类型的日期字段等。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2、数据一致性。同样，数据源系统为了性能的考虑，会在一定程度上舍弃外键约束，这通常会导致数据不一致。例如在帐务表中会出现一个用户表中没有的用户ID，在例如有些代码在代码表中找不到等。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3、业务逻辑的合理性。这一点很难说对与错。通常，数据源系统的设计并不是非常严谨，例如让用户开户日期晚于用户销户日期都是有可能发生的，一个用户表中存在多个用户ID也是有可能发生的。对这种情况，有什么办法吗？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;构建一个BI系统，要做到完全理解数据源系统根本就是不可能的。特别是数据源系统在交付后，有更多维护人员的即兴发挥，那更是要花大量的时间去寻找原因。以前曾经争辩过设计人员对规则描述的问题，有人提出要在ETL开始之前务必将所有的规则弄得一清二楚。我并不同意这样的意见，倒是认为在ETL过程要有处理这些质量有问题数据的保证。一定要正面这些脏数据，是丢弃还是处理，无法逃避。如果没有质量保证，那么在这个过程中，错误会逐渐放大，抛开数据源质量问题，我们再来看看ETL过程中哪些因素对数据准确性产生重大影响。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1、规则描述错误。上面提到对设计人员对数据源系统理解的不充分，导致规则理解错误，这是一方面。另一方面，是规则的描述，如果无二义性地描述规则也是要探求的一个课题。规则是依附于目标字段的，在探求之三中，提到规则的分类。但是规则总不能总是用文字描述，必须有严格的数学表达方式。我甚至想过，如果设计人员能够使用某种规则语言来描述，那么我们的ETL单元就可以自动生成、同步，省去很多手工操作了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2、ETL开发错误。即时规则很明确，ETL开发的过程中也会发生一些错误，例如逻辑错误、书写错误等。例如对于一个分段值，开区间闭区间是需要指定的，但是常常开发人员没注意，一个大于等于号写成大于号就导致数据错误。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3、人为处理错误。在整体ETL流程没有完成之前，为了图省事，通常会手工运行ETL过程，这其中一个重大的问题就是你不会按照正常流程去运行了，而是按照自己的理解去运行，发生的错误可能是误删了数据、重复装载数据等。 探求ETL本质之五(质量保证)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上回提到ETL数据质量问题，这是无法根治的，只能采取特定的手段去尽量避免，而且必须要定义出度量方法来衡量数据的质量是好还是坏。对于数据源的质量，客户对此应该更加关心，如果在这个源头不能保证比较干净的数据，那么后面的分析功能的可信度也都成问题。数据源系统也在不断进化过程中，客户的操作也在逐渐规范中，BI系统也同样如此。本文探讨一下对数据源质量和ETL处理质量的应对方法。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如何应对数据源的质量问题？记得在onteldatastage列表中也讨论过一个话题——“-1的处理”，在数据仓库模型维表中，通常有一条-1记录，表示“未知”，这个未知含义可广了，任何可能出错的数据，NULL数据甚至是规则没有涵盖到的数据，都转成-1。这是一种处理脏数据的方法，但这也是一种掩盖事实的方法。就好像写一个函数FileOpen(filename)，返回一个错误码，当然，你可以只返回一种错误码，如-1，但这是一种不好的设计，对于调用者来说，他需要依据这个错误码进行某些判断，例如是文件不存在，还是读取权限不够，都有相应的处理逻辑。数据仓库中也是一样，所以，建议将不同的数据质量类型处理结果分别转换成不同的值，譬如，在转换后，-1表示参照不上，-2表示NULL数据等。不过这仅仅对付了上回提到的第一类错误，数据格式错误。对于数据一致性和业务逻辑合理性问题，这仍有待探求。但这里有一个原则就是“必须在数据仓库中反应数据源的质量”。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于ETL过程中产生的质量问题，必须有保障手段。从以往的经验看，没有保障手段给实施人员带来麻烦重重。实施人员对于反复装载数据一定不会陌生，甚至是最后数据留到最后的Cube，才发现了第一步ETL其实已经错了。这个保障手段就是数据验证机制，当然，它的目的是能够在ETL过程中监控数据质量，产生报警。这个模块要将实施人员当作是最终用户，可以说他们是数据验证机制的直接收益者。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先，必须有一个对质量的度量方法，什么是高质什么是低质，不能靠感官感觉，但这却是在没有度量方法条件下通常的做法。那经营分析系统来说，联通总部曾提出测试规范，这其实就是一种度量方法，例如指标的误差范围不能高于5%等，对系统本身来说其实必须要有这样的度量方法，先不要说这个度量方法是否科学。对于ETL数据处理质量，他的度量方法应该比联通总部测试规范定义的方法更要严格，因为他更多将BI系统看作一个黑盒子，从数据源到展现的数据误差允许一定的误差。而ETL数据处理质量度量是一种白盒的度量，要注重每一步过程。因此理论上，要求输入输出的指标应该完全一致。但是我们必须正面完全一致只是理想，对于有误差的数据，必须找到原因。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在质量度量方法的前提下，就可以建立一个数据验证框架。此框架依据总量、分量数据稽核方法，该方法在高的《数据仓库中的数据稽核技术》一文中已经指出。作为补充，下面提出几点功能上的建议：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1、提供前端。将开发实施人员当作用户，同样也要为之提供友好的用户界面。《稽核技术》一文中指出测试报告的形式，这种形式还是要依赖人为判断，在一堆数据中去找规律。到不如用OLAP的方式提供界面，不光是加上测试统计出来的指标结果，并且配合度量方法的计算。例如误差率，对于误差率为大于0的指标，就要好好查一下原因了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2、提供框架。数据验证不是一次性工作，而是每次ETL过程中都必须做的。因此，必须有一个框架，自动化验证过程，并提供扩展手段，让实施人员能够增加验证范围。有了这样一个框架，其实它起到规范化操作的作用，开发实施人员可以将主要精力放在验证脚本的编写上，而不必过多关注验证如何融合到流程中，如何展现等工作。为此，要设计一套表，类似于DM表，每次验证结果数据都记录其中，并且自动触发多维分析的数据装载、发布等。这样，实施人员可以在每次装载，甚至在流程过程中就可以观察数据的误差率。特别是，如果数据仓库的模型能够统一起来，甚至数据验证脚本都可以确定下来，剩下的就是规范流程了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3、规范流程。上回提到有一种ETL数据质量问题是由于人工处理导致的，其中最主要原因还是流程不规范。开发实施人员运行单独一个ETL单元是很方便的，虽然以前曾建议一个ETL单元必须是”可重入”的，这能够解决误删数据，重复装载数据问题。但要记住数据验证也是在流程当中，要让数据验证能够日常运作，就不要让实施者感觉到他的存在。总的来说，规范流程是提高实施效率的关键工作，这也是以后要继续探求的。 探求ETL本质之六(元数据漫谈)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于元数据(Metadata)的定义到目前为止没有什么特别精彩的，这个概念非常广，一般都是这样定义，“元数据是描述数据的数据(Data about Data)”，这造成一种递归定义，就像问小强住在哪里，答，在旺财隔壁。按照这样的定义，元数据所描述的数据是什么呢？还是元数据。这样就可能有元元元…元数据。我还听说过一种对元数据，如果说数据是一抽屉档案，那么元数据就是分类标签。那它和索引有什么区别？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;元数据体现是一种抽象，哲学家从古至今都在抽象这个世界，力图找到世界的本质。抽象不是一层关系，它是一种逐步由具体到一般的过程。例如我-&gt;男人-&gt;人-&gt;哺乳动物-&gt;生物这就是一个抽象过程，你要是在软件业混会发现这个例子很常见，面向对象方法就是这样一种抽象过程。它对世界中的事物、过程进行抽象，使用面向对象方法，构建一套对象模型。同样在面向对象方法中，类是对象的抽象，接口又是对类的抽象。因此，我认为可以将“元”和“抽象”换一下，叫抽象数据是不是好理解一些。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;常听到这样的话，“xx领导的讲话高屋建瓴，给我们后面的工作指引的清晰的方向”，这个成语“高屋建瓴”，站在10楼往下倒水，居高临下，能砸死人，这是指站在一定的高度看待事物，这个一定的高度就是指他有够“元”。在设计模式中，强调要对接口编程，就是说你不要处理这类对象和那类对象的交互，而要处理这个接口和那个接口的交互，先别管他们内部是怎么干的。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;元数据存在的意义也在于此，虽然上面说了一通都扯到哲学上去，但这个词必须还是要结合软件设计中看，我不知道在别的领域是不是存在Metadata这样的叫法，虽然我相信别的领域必然有类似的东东。元数据的存在就是要做到在更高抽象一层设计软件。这肯定有好处，什么灵活性啊，扩展性啊，可维护性啊，都能得到提高，而且架构清晰，只是弯弯太多，要是从下往上看，太复杂了。很早以前，我曾看过backorifice的代码，我靠，一个简单的功能，从这个类转到父类，又转到父类，很不理解，为什么一个简单的功能不在一个类的方法中实现就拉到了呢？现在想想，还真不能这样，这虽然使代码容易看懂了，但是结构确是混乱的，那他只能干现在的事，如果有什么功能扩展，这些代码就废了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我从98年刚工作时就开始接触元数据的概念，当时叫做元数据驱动的系统架构，后来在QiDSS中也用到这个概念构建QiNavigator，但是现在觉得元数据也没啥，不就是建一堆表描述界面的元素，再利用这些数据自动生成界面吗。到了数据仓库系统中，这个概念更强了，是数据仓库中一个重要的部分。但是至今，我还是认为这个概念过于玄乎，看不到实际的东西，市面上有一些元数据管理的东西，但是从应用情况就得知，用的不多。之所以玄乎，就是因为抽象层次没有分清楚，关键就是对于元数据的分类（这种分类就是一种抽象过程）和元数据的使用。你可以将元数据抽象成0和1，但是那样对你的业务有用吗？必须还得抽象到适合的程度，最后问题还是“度”。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库系统的元数据作用如何？还不就是使系统自动运转，易于管理吗？要做到这一步，可没必要将系统抽象到太极、两仪、八卦之类的，业界也曾定义过一些元数据规范，向CWM、XMI等等，可以借鉴，不过俺对此也是不精通的说，以后再说。 原文：https://blog.csdn.net/zhongguomao/article/details/53769919]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>ETL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[virtualbox虚拟机安装Centos 6]]></title>
    <url>%2F2018%2F09%2F17%2Fvirtualbox-centOS%2F</url>
    <content type="text"><![CDATA[第一部分：安装前的准备工作步骤1：点击“新建”按钮，创建一个新虚拟机。步骤2：给虚拟机命名，选择操作系统及版本，一般命名后操作系统会自动带出步骤3：选择内存大小，我这里设置的是1.5G步骤4：选择现在创建虚拟硬盘步骤5：选择虚拟硬盘的类型，VDI是VirtualBox专用，VMDK可以和VM通用，这里我们选VDI步骤6：选择动态分配磁盘容量步骤7：选择文件存储的位置及容量大小，点击创建完成 第二部分：开始正式安装系统步骤1：选择刚创建的虚拟机，然后点击“启动”按钮。步骤2：选择我们下载的CentOS安装盘步骤3：确定后，进入CentOS6.8的安装引导页面，由于服务器一般不需要太高显卡，我们使用第二项基本的显卡驱动安装系统，也可以选择第一个项。步骤4：是否对电脑媒介进行检测，选skip跳过检测。步骤5：跳过硬盘检测后进入图形安装界面，点击下一步步骤6：选择安装过程中使用的语言，建议选择英文，后期会减少很多麻烦。步骤7：选择键盘类型，美式键盘。步骤8：使用默认“基本储存设备”，点击【下一步】。步骤9：选择是否删除虚拟磁盘里数据，选择“是，忽略所有数据”步骤10：输入主机名和配置网络，网络可以在系统安装完成后在配置。步骤11：选择时区，这里我们选上海步骤12：设置root用户密码，尽量复杂一些步骤13：接下来是对磁盘进行分区操作，建议选自定义步骤14：选择Create进行分区工作步骤15：在这里我们选择创建标准分区（生产环境专业人员建议选逻辑分区）步骤16：创建交换分区swap，交换分区没有挂载点。 网上建议交换分区大小一般是2倍内存，实际上没有任何意义，这是一个误区，以前老机器内存小，现在生产服务器一般是16g或32g内存，这里我们设置了4G，生产环境最多最多8G或16G就ok了 步骤17：我们已经设置好一个交换分区，接下来继续创建步骤18：创建根分区，我们把剩余的空间全部创建成根分区。 其他教程linux要创建很多分区，像根分区，boot分区，home分区等等，实际上没有必要。因为如果只有一个根分区的话，其他的目录实际上都是在根分区下面，我们为boot或home单独创建一个分区它就会单独挂在这个分区上，不创建它就全部在根分区上面，所以我们只需要创建一个根分区就可以了，除非你的上层环境有特殊要求否则没必要单独分开。步骤19：分区完成创建后继续下一步步骤20：系统创建并格式化各个分区步骤21：选择“Write changes to disk”，写入硬盘步骤22：设置引导，默认即可（默认mbr引导方式）同时这里还可以给引导加密码，防止有人通过引导进入单用户模式清除root密码，会更加安全一点，一般我们就不加了步骤23：选择我们安装的软件，初学者使用的话建议选择Desktop桌面环境，会安装图形界面，当然后面也可以改步骤24：开始安装安装进度上可以看到红帽的发行版本”el6”，能看出centos实际上是从红帽来的步骤25：安装完毕，提示我们重启步骤26：重启系统 第三部分：首次进入系统步骤1：首次启动会有一些基本设置，我们点击下一步步骤2：同意版权信息，下一步步骤3：创建用户，系统已经存在root用户了，但是平时尽量不要使用root用户，这里是创建普通用户。我创建了一个admin用户，忘截图了，填完用户信息，下一步。步骤4：选择时间，生产环境建议勾上与网络时间同步，这里由于我这没有网络环境就不勾选了步骤5：配置kdump，由于我是虚拟机系统提示我内存不足，不配不影响，OK继续。kdump是在系统崩溃、死锁或者死机的时候，它会帮助我们把当时内存cope下来，可以做分析。一般我们不会分析，可以发给红帽的专业人士，配置完成后我们重启。步骤6：进入登录界面，到此安装完成。原文：https://blog.csdn.net/seeseait/article/details/52312195]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>virtualbox</tag>
        <tag>centOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql替换字段中部分字符串]]></title>
    <url>%2F2018%2F09%2F06%2Fmysql-replace%2F</url>
    <content type="text"><![CDATA[将testsql 表中的pic_url 字段中的所有 aaaa字符串替换成bbbb ; UPDATE testsql set pic_url=REPLACE(pic_url,’aaaa’,’bbbb’); 以上是网上百度的，下面sql经实际运行有效 UPDATE FUNCTION set authorization_uri=REPLACE(authorization_uri,’172.23.6.115:50080’,’172.23.7.197:50080’);UPDATE FUNCTION set uri=REPLACE(uri,’172.23.6.115:50080’,’172.23.7.197:50080’);]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm 学习（二）起步]]></title>
    <url>%2F2018%2F09%2F05%2Fstorm-02%2F</url>
    <content type="text"><![CDATA[准备开始在本章，我们要创建一个 Storm 工程和我们的第一个 Storm 拓扑结构。 NOTE: 下面假设你的 JRE 版本在 1.6 以上。我们推荐 Oracle 提供的 JRE。你可以到 http://www.java.com/downloads/ 下载。 操作模式开始之前，有必要了解一下 Storm 的操作模式。有下面两种方式。 本地模式在本地模式下，Storm 拓扑结构运行在本地计算机的单一 JVM 进程上。这个模式用于开发、测试以及调试，因为这是观察所有组件如何协同工作的最简单方法。在这种模式下，我们可以调整参数，观察我们的拓扑结构如何在不同的 Storm 配置环境下运行。要在本地模式下运行，我们要下载 Storm 开发依赖，以便用来开发并测试我们的拓扑结构。我们创建了第一个 Storm 工程以后，很快就会明白如何使用本地模式了。 NOTE: 在本地模式下，跟在集群环境运行很像。不过很有必要确认一下所有组件都是线程安全的，因为当把它们部署到远程模式时它们可能会运行在不同的 JVM 进程甚至不同的物理机上，这个时候它们之间没有直接的通讯或共享内存。 我们要在本地模式运行本章的所有例子。 远程模式在远程模式下，我们向 Storm 集群提交拓扑，它通常由许多运行在不同机器上的流程组成。远程模式不会出现调试信息， 因此它也称作生产模式。不过在单一开发机上建立一个 Storm 集群是一个好主意，可以在部署到生产环境之前，用来确认拓扑在集群环境下没有任何问题。 你将在第六章学到更多关于远程模式的内容，并在附录B学到如何安装一个 Storm 集群。 Hello World我们在这个工程里创建一个简单的拓扑，数单词数量。我们可以把这个看作 Storm 的 “Hello World”。不过，这是一个非常强大的拓扑，因为它能够扩展到几乎无限大的规模，而且只需要做一些小修改，就能用它构建一个统计系统。举个例子，我们可以修改一下工程用来找出 Twitter 上的热点话题。 要创建这个拓扑，我们要用一个 spout 读取文本，第一个 bolt 用来标准化单词，第二个 bolt 为单词计数，如图2-1所示。你可以从这个网址下载源码压缩包， https://github.com/storm-book/examples-ch02-getting_started/zipball/master。 NOTE: 如果你使用 git（一个分布式版本控制与源码管理工具），你可以执行 git clone git@github.com:storm-book/examples-ch02-getting_started.git，把源码检出到你指定的目录。 Java 安装检查 构建 Storm 运行环境的第一步是检查你安装的 Java 版本。打开一个控制台窗口并执行命令：java -version。控制台应该会显示出类似如下的内容：123456java -versionjava version "1.6.0_26"Java(TM) SE Runtime Enviroment (build 1.6.0_26-b03)Java HotSpot(TM) Server VM (build 20.1-b02, mixed mode) 如果不是上述内容，检查你的 Java 安装情况。（参考 http://www.java.com/download/） 创建工程开始之前，先为这个应用建一个目录（就像你平常为 Java 应用做的那样）。这个目录用来存放工程源码。 接下来我们要下载 Storm 依赖包，这是一些 jar 包，我们要把它们添加到应用类路径中。你可以采用如下两种方式之一完成这一步： 下载所有依赖，解压缩它们，把它 们添加到类路径 使用 Apache Maven NOTE: Maven 是一个软件项目管理的综合工具。它可以用来管理项目的开发周期的许多方面，从包依赖到版本发布过程。在这本书中，我们将广泛使用它。如果要检查是否已经安装了maven，在命令行运行 mvn。如果没有安装你可以从 http://maven.apache.org/download.html下载。 没有必要先成为一个 Maven 专家才能使用 Storm，不过了解一下关于 Maven 工作方式的基础知识仍然会对你有所帮助。你可以在 Apache Maven 的网站上找到更多的信息（http://maven.apache.org/）。 NOTE: Storm 的 Maven 依赖引用了运行 Storm 本地模式的所有库。 要运行我们的拓扑，我们可以编写一个包含基本组件的 pom.xml 文件。1234567891011121314151617181920212223242526272829303132333435363738&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;storm.book&lt;/groupId&gt; &lt;artifactId&gt;Getting-Started&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.6&lt;/source&gt; &lt;target&gt;1.6&lt;/target&gt; &lt;compilerVersion&gt;1.6&lt;/compilerVersion&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;clojars.org&lt;/id&gt; &lt;url&gt;http://clojars.org/repo&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;storm&lt;/groupId&gt; &lt;artifactId&gt;storm&lt;/artifactId&gt; &lt;version&gt;0.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 开头几行指定了工程名称和版本号。然后我们添加了一个编译器插件，告知 Maven 我们的代码要用 Java1.6 编译。接下来我们定义了 Maven 仓库（Maven 支持为同一个工程指定多个仓库）。clojars 是存放 Storm 依赖的仓库。Maven 会为运行本地模式自动下载必要的所有子包依赖。 一个典型的 Maven Java 工程会拥有如下结构：12345678我们的应用目录/ ├── pom.xml └── src └── main └── java | ├── spouts | └── bolts └── resources java 目录下的子目录包含我们的代码，我们把要统计单词数的文件保存在 resource 目录下。 NOTE：命令 mkdir -p 会创建所有需要的父目录。 创建我们的第一个 Topology我们将为运行单词计数创建所有必要的类。可能这个例子中的某些部分，现在无法讲的很清楚，不过我们会在随后的章节做进一步的讲解。Spoutpout WordReader 类实现了 IRichSpout 接口。我们将在第四章看到更多细节。WordReader负责从文件按行读取文本，并把文本行提供给第一个 bolt。 NOTE: 一个 spout 发布一个定义域列表。这个架构允许你使用不同的 bolts 从同一个spout 流读取数据，它们的输出也可作为其它 bolts 的定义域，以此类推。 例2-1包含 WordRead 类的完整代码（我们将会分析下述代码的每一部分）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980/ 例2-1.src/main/java/spouts/WordReader.java / package spouts; import java.io.BufferedReader; import java.io.FileNotFoundException; import java.io.FileReader; import java.util.Map; import backtype.storm.spout.SpoutOutputCollector; import backtype.storm.task.TopologyContext; import backtype.storm.topology.IRichSpout; import backtype.storm.topology.OutputFieldsDeclarer; import backtype.storm.tuple.Fields; import backtype.storm.tuple.Values; public class WordReader implements IRichSpout &#123; private SpoutOutputCollector collector; private FileReader fileReader; private boolean completed = false; private TopologyContext context; public boolean isDistributed() &#123;return false;&#125; public void ack(Object msgId) &#123; System.out.println("OK:"+msgId); &#125; public void close() &#123;&#125; public void fail(Object msgId) &#123; System.out.println("FAIL:"+msgId); &#125; / 这个方法做的惟一一件事情就是分发文件中的文本行 / public void nextTuple() &#123; / 这个方法会不断的被调用，直到整个文件都读完了，我们将等待并返回。 / if(completed)&#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; //什么也不做 &#125; return; &#125; String str; //创建reader BufferedReader reader = new BufferedReader(fileReader); try&#123; //读所有文本行 while((str = reader.readLine()) != null)&#123; / 按行发布一个新值 / this.collector.emit(new Values(str),str); &#125; &#125;catch(Exception e)&#123; throw new RuntimeException("Error reading tuple",e); &#125;finally&#123; completed = true; &#125; &#125; / 我们将创建一个文件并维持一个collector对象 / public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) &#123; try &#123; this.context = context; this.fileReader = new FileReader(conf.get("wordsFile").toString()); &#125; catch (FileNotFoundException e) &#123; throw new RuntimeException("Error reading file ["+conf.get("wordFile")+"]"); &#125; this.collector = collector; &#125; / 声明输入域"word" / public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("line")); &#125; &#125; 第一个被调用的 spout 方法都是 public void open(Map conf, TopologyContext context, SpoutOutputCollector collector)。它接收如下参数：配置对象，在定义topology 对象是创建；TopologyContext 对象，包含所有拓扑数据；还有SpoutOutputCollector 对象，它能让我们发布交给 bolts 处理的数据。下面的代码主是这个方法的实现。12345678910public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) &#123; try &#123; this.context = context; this.fileReader = new FileReader(conf.get("wordsFile").toString()); &#125; catch (FileNotFoundException e) &#123; throw new RuntimeException("Error reading file ["+conf.get("wordFile")+"]"); &#125; this.collector = collector;&#125; 我们在这个方法里创建了一个 FileReader 对象，用来读取文件。接下来我们要实现 public void nextTuple()，我们要通过它向 bolts 发布待处理的数据。在这个例子里，这个方法要读取文件并逐行发布数据。123456789101112131415161718192021public void nextTuple() &#123; if(completed)&#123; try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; //什么也不做 &#125; return; &#125; String str; BufferedReader reader = new BufferedReader(fileReader); try&#123; while((str = reader.readLine()) != null)&#123; this.collector.emit(new Values(str)); &#125; &#125;catch(Exception e)&#123; throw new RuntimeException("Error reading tuple",e); &#125;finally&#123; completed = true; &#125;&#125; NOTE: Values 是一个 ArrarList 实现，它的元素就是传入构造器的参数。 nextTuple() 会在同一个循环内被 ack() 和 fail() 周期性的调用。没有任务时它必须释放对线程的控制，其它方法才有机会得以执行。因此 nextTuple 的第一行就要检查是否已处理完成。如果完成了，为了降低处理器负载，会在返回前休眠一毫秒。如果任务完成了，文件中的每一行都已被读出并分发了。 NOTE:元组(tuple)是一个具名值列表，它可以是任意 java 对象（只要它是可序列化的）。默认情况，Storm 会序列化字符串、字节数组、ArrayList、HashMap 和 HashSet 等类型。 Bolts现在我们有了一个 spout，用来按行读取文件并每行发布一个元组，还要创建两个 bolts，用来处理它们（看图2-1）。bolts 实现了接口 backtype.storm.topology.IRichBolt。 bolt最重要的方法是void execute(Tuple input)，每次接收到元组时都会被调用一次，还会再发布若干个元组。 NOTE: 只要必要，bolt 或 spout 会发布若干元组。当调用 nextTuple 或 execute 方法时，它们可能会发布0个、1个或许多个元组。你将在第五章学习更多这方面的内容。 第一个 bolt，WordNormalizer，负责得到并标准化每行文本。它把文本行切分成单词，大写转化成小写，去掉头尾空白符。 首先我们要声明 bolt 的出参：123public void declareOutputFields(OutputFieldsDeclarer declarer)&#123; declarer.declare(new Fields("word"));&#125; 这里我们声明 bolt 将发布一个名为 “word” 的域。 下一步我们实现 public void execute(Tuple input)，处理传入的元组：1234567891011121314public void execute(Tuple input)&#123; String sentence=input.getString(0); String[] words=sentence.split(" "); for(String word : words)&#123; word=word.trim(); if(!word.isEmpty())&#123; word=word.toLowerCase(); //发布这个单词 collector.emit(new Values(word)); &#125; &#125; //对元组做出应答 collector.ack(input);&#125; 第一行从元组读取值。值可以按位置或名称读取。接下来值被处理并用collector对象发布。最后，每次都调用collector 对象的 ack() 方法确认已成功处理了一个元组。 例2-2是这个类的完整代码。12345678910111213141516171819202122232425262728293031323334353637383940414243444546//例2-2 src/main/java/bolts/WordNormalizer.javapackage bolts;import java.util.ArrayList;import java.util.List;import java.util.Map;import backtype.storm.task.OutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.IRichBolt;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.tuple.Fields;import backtype.storm.tuple.Tuple;import backtype.storm.tuple.Values;public class WordNormalizer implements IRichBolt&#123; private OutputCollector collector; public void cleanup()&#123;&#125; / bolt从单词文件接收到文本行，并标准化它。 文本行会全部转化成小写，并切分它，从中得到所有单词。 / public void execute(Tuple input)&#123; String sentence = input.getString(0); String[] words = sentence.split(" "); for(String word : words)&#123; word = word.trim(); if(!word.isEmpty())&#123; word=word.toLowerCase(); //发布这个单词 List a = new ArrayList(); a.add(input); collector.emit(a,new Values(word)); &#125; &#125; //对元组做出应答 collector.ack(input); &#125; public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; this.collector=collector; &#125; / 这个bolt只会发布“word”域 / public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("word")); &#125;&#125; NOTE:通过这个例子，我们了解了在一次 execute 调用中发布多个元组。如果这个方法在一次调用中接收到句子 “This is the Storm book”，它将会发布五个元组。 下一个bolt，WordCounter，负责为单词计数。这个拓扑结束时（cleanup() 方法被调用时），我们将显示每个单词的数量。 NOTE: 这个例子的 bolt 什么也没发布，它把数据保存在 map 里，但是在真实的场景中可以把数据保存到数据库。12345678910111213141516171819202122232425262728293031323334353637383940414243444546package bolts;import java.util.HashMap;import java.util.Map;import backtype.storm.task.OutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.IRichBolt;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.tuple.Tuple;public class WordCounter implements IRichBolt&#123; Integer id; String name; Map counters; private OutputCollector collector; / 这个spout结束时（集群关闭的时候），我们会显示单词数量 / @Override public void cleanup()&#123; System.out.println("-- 单词数 【"+name+"-"+id+"】 --"); for(Map.Entry entry : counters.entrySet())&#123; System.out.println(entry.getKey()+": "+entry.getValue()); &#125; &#125; / 为每个单词计数 /@Overridepublic void execute(Tuple input) &#123; String str=input.getString(0); /** 如果单词尚不存在于map，我们就创建一个，如果已在，我们就为它加1 / if(!counters.containsKey(str))&#123; counters.put(str,1); &#125;else&#123; Integer c = counters.get(str) + 1; counters.put(str,c); &#125; //对元组作为应答 collector.ack(input);&#125; / 初始化 / @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector)&#123; this.counters = new HashMap(); this.collector = collector; this.name = context.getThisComponentId(); this.id = context.getThisTaskId(); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123;&#125;&#125; execute 方法使用一个 map 收集单词并计数。拓扑结束时，将调用 clearup() 方法打印计数器 map。（虽然这只是一个例子，但是通常情况下，当拓扑关闭时，你应当使用 cleanup() 方法关闭活动的连接和其它资源。） 主类你可以在主类中创建拓扑和一个本地集群对象，以便于在本地测试和调试。LocalCluster 可以通过 Config 对象，让你尝试不同的集群配置。比如，当使用不同数量的工作进程测试你的拓扑时，如果不小心使用了某个全局变量或类变量，你就能够发现错误。（更多内容请见第三章） NOTE：所有拓扑节点的各个进程必须能够独立运行，而不依赖共享数据（也就是没有全局变量或类变量），因为当拓扑运行在真实的集群环境时，这些进程可能会运行在不同的机器上。 接下来，TopologyBuilder 将用来创建拓扑，它决定 Storm 如何安排各节点，以及它们交换数据的方式。 1234TopologyBuilder builder = new TopologyBuilder();builder.setSpout("word-reader", new WordReader());builder.setBolt("word-normalizer", new WordNormalizer()).shuffleGrouping("word-reader");builder.setBolt("word-counter", new WordCounter()).shuffleGrouping("word-normalizer"); 在 spout 和 bolts 之间通过 shuffleGrouping 方法连接。这种分组方式决定了 Storm 会以随机分配方式从源节点向目标节点发送消息。 下一步，创建一个包含拓扑配置的 Config 对象，它会在运行时与集群配置合并，并通过prepare 方法发送给所有节点。 123Config conf = new Config();conf.put("wordsFile", args[0]);conf.setDebug(true); 由 spout 读取的文件的文件名，赋值给 wordFile 属性。由于是在开发阶段，设置 debug 属性为 true，Strom 会打印节点间交换的所有消息，以及其它有助于理解拓扑运行方式的调试数据。 正如之前讲过的，你要用一个 LocalCluster 对象运行这个拓扑。在生产环境中，拓扑会持续运行，不过对于这个例子而言，你只要运行它几秒钟就能看到结果。1234LocalCluster cluster = new LocalCluster();cluster.submitTopology("Getting-Started-Topologie", conf, builder.createTopology());Thread.sleep(2000);cluster.shutdown(); 调用 createTopology 和 submitTopology，运行拓扑，休眠两秒钟（拓扑在另外的线程运行），然后关闭集群。 例2-3是完整的代码123456789101112131415161718192021222324252627282930//例2-3 src/main/java/TopologyMain.javaimport spouts.WordReader;import backtype.storm.Config;import backtype.storm.LocalCluster;import backtype.storm.topology.TopologyBuilder;import backtype.storm.tuple.Fields;import bolts.WordCounter;import bolts.WordNormalizer;public class TopologyMain &#123; public static void main(String[] args) throws InterruptedException &#123; //定义拓扑 TopologyBuilder builder = new TopologyBuilder()); builder.setSpout("word-reader", new WordReader()); builder.setBolt("word-normalizer", new WordNormalizer()).shuffleGrouping("word-reader"); builder.setBolt("word-counter", new WordCounter(),2).fieldsGrouping("word-normalizer", new Fields("word")); //配置 Config conf = new Config(); conf.put("wordsFile", args[0]); conf.setDebug(false); //运行拓扑 conf.put(Config.TOPOLOGY_MAX_SPOUT_PENDING, 1); LocalCluster cluster = new LocalCluster(); cluster.submitTopology("Getting-Started-Topologie", conf, builder.createTopology(); Thread.sleep(1000); cluster.shutdown(); &#125;&#125; 观察运行情况 你已经为运行你的第一个拓扑准备好了。在这个目录下面创建一个文件，/src/main/resources/words.txt，一个单词一行，然后用下面的命令运行这个拓扑：mvn exec:java -Dexec.mainClass=”TopologyMain” -Dexec.args=”src/main/resources/words.txt。举个例子，如果你的 words.txt 文件有如下内容： Storm test are great is an Storm simple application but very powerful really Storm is great 你应该会在日志中看到类似下面的内容： is: 2 application: 1 but: 1 great: 1 test: 1 simple: 1 Storm: 3 really: 1 are: 1 great: 1 an: 1 powerful: 1 very: 1 在这个例子中，每类节点只有一个实例。但是如果你有一个非常大的日志文件呢？你能够很轻松的改变系统中的节点数量实现并行工作。这个时候，你就要创建两个 WordCounter* 实例。1builder.setBolt("word-counter", new WordCounter(),2).shuffleGrouping("word-normalizer"); 程序返回时，你将看到： — 单词数 【word-counter-2】 — application: 1 is: 1 great: 1 are: 1 powerful: 1 Storm: 3 — 单词数 [word-counter-3] — really: 1 is: 1 but: 1 great: 1 test: 1 simple: 1 an: 1 very: 1 棒极了！修改并行度实在是太容易了（当然对于实际情况来说，每个实例都会运行在单独的机器上）。不过似乎有一个问题：单词 is 和 great 分别在每个 WordCounter 各计数一次。怎么会这样？当你调用shuffleGrouping 时，就决定了 Storm 会以随机分配的方式向你的 bolt 实例发送消息。在这个例子中，理想的做法是相同的单词问题发送给同一个 WordCounter 实例。你把shuffleGrouping(“word-normalizer”) 换成 fieldsGrouping(“word-normalizer”, new Fields(“word”)) 就能达到目的。试一试，重新运行程序，确认结果。 你将在后续章节学习更多分组方式和消息流类型。 结论我们已经讨论了 Storm 的本地和远程操作模式之间的不同，以及 Storm 的强大和易于开发的特性。你也学习了一些 Storm 的基本概念，我们将在后续章节深入讲解它们。]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm 学习（一） 基础知识]]></title>
    <url>%2F2018%2F09%2F05%2Fstorm-01%2F</url>
    <content type="text"><![CDATA[基础知识Storm 是一个分布式的，可靠的，容错的数据流处理系统。它会把工作任务委托给不同类型的组件，每个组件负责处理一项简单特定的任务。Storm 集群的输入流由一个被称作 spout 的组件管理，spout 把数据传递给 bolt， bolt 要么把数据保存到某种存储器，要么把数据传递给其它的 bolt。你可以想象一下，一个 Storm 集群就是在一连串的 bolt 之间转换 spout 传过来的数据。 这里用一个简单的例子来说明这个概念。昨晚我在新闻节目里看到主持人在谈论政治人物和他们对于各种政治话题的立场。他们一直重复着不同的名字，而我开始考虑这些名字是否被提到了相同的次数，以及不同次数之间的偏差。 想像播音员读的字幕作为你的数据输入流。你可以用一个 spout 读取一个文件（或者 socket，通过 HTTP，或者别的方法）。文本行被 spout 传给一个 bolt，再被 bolt 按单词切割。单词流又被传给另一个 bolt，在这里每个单词与一张政治人名列表比较。每遇到一个匹配的名字，第二个 bolt 为这个名字在数据库的计数加1。你可以随时查询数据库查看结果， 而且这些计数是随着数据到达实时更新的。 现在想象一下，很容易在整个 Storm 集群定义每个 bolt 和 spout 的并行性级别，因此你可以无限的扩展你的拓扑结构。很神奇，是吗？尽管这是个简单例子，你也可以看到 Storm 的强大。 有哪些典型的 Storm 应用案例？ 数据处理流 正如上例所展示的，不像其它的流处理系统，Storm 不需要中间队列。 连续计算 连续发送数据到客户端，使它们能够实时更新并显示结果，如网站指标。 分布式远程过程调用 频繁的 CPU 密集型操作并行化。 Storm 组件 对于一个Storm集群，一个连续运行的主节点组织若干节点工作。 在 Storm 集群中，有两类节点：主节点 master node 和工作节点 worker nodes。主节点运行着一个叫做 Nimbus 的守护进程。这个守护进程负责在集群中分发代码，为工作节点分配任务，并监控故障。Supervisor守护进程作为拓扑的一部分运行在工作节点上。一个 Storm 拓扑结构在不同的机器上运行着众多的工作节点。 因为 Storm 在 Zookeeper 或本地磁盘上维持所有的集群状态，守护进程可以是无状态的而且失效或重启时不会影响整个系统的健康 在系统底层，Storm 使用了 zeromq(0mq, zeromq(http://www.zeromq.org))。这是一种先进的，可嵌入的网络通讯库，它提供的绝妙功能使 Storm 成为可能。下面列出一些 zeromq 的特性。 一个并发架构的 Socket 库 对于集群产品和超级计算，比 TCP 要快 可通过 inproc（进程内）, IPC（进程间）, TCP 和 multicast(多播协议)通信 异步 I / O 的可扩展的多核消息传递应用程序 利用扇出(fanout), 发布订阅（PUB-SUB）,管道（pipeline）, 请求应答（REQ-REP），等方式实现 N-N 连接 NOTE: Storm 只用了 push/pull socketsStorm 的特性在所有这些设计思想与决策中，有一些非常棒的特性成就了独一无二的 Storm。 简化编程：如果你曾试着从零开始实现实时处理，你应该明白这是一件多么痛苦的事情。使用 Storm，复杂性被大大降低了。 使用一门基于 JVM 的语言开发会更容易，但是你可以借助一个小的中间件，在 Storm 上使用任何语言开发。有现成的中间件可供选择，当然也可以自己开发中间件。 容错：Storm 集群会关注工作节点状态，如果宕机了必要的时候会重新分配任务。 可扩展：所有你需要为扩展集群所做的工作就是增加机器。Storm 会在新机器就绪时向它们分配任务。 可靠的：所有消息都可保证至少处理一次。如果出错了，消息可能处理不只一次，不过你永远不会丢失消息。 快速：速度是驱动 Storm 设计的一个关键因素 事务性：You can get exactly once messaging semantics for pretty much any computation. 你可以为几乎任何计算得到恰好一次消息语义。]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用git将项目上传到github]]></title>
    <url>%2F2018%2F09%2F03%2Fgit-new%2F</url>
    <content type="text"><![CDATA[使用git将项目上传到github（最简单方法）首先你需要一个github账号，所有还没有的话先去注册吧！我们使用git需要先安装git工具，这里给出下载地址，下载后一路直接安装即可： 进入Github首页，点击New repository新建一个项目 填写相应信息后点击create即可Repository name: 仓库名称Description(可选): 仓库描述介绍Public, Private : 仓库权限（公开共享，私有或指定合作者）Initialize this repository with a README: 添加一个README.mdgitignore: 不需要进行版本管理的仓库类型，对应生成文件.gitignorelicense: 证书类型，对应生成文件LICENSE 点击Clone or dowload会出现一个地址，copy这个地址备用。 接下来就到本地操作了，首先右键你的项目，如果你之前安装git成功的话，右键会出现两个新选项，分别为Git Gui Here,Git Bash Here,这里我们选择Git Bash Here，进入如下界面，blog-hexo即为我的项目名。 接下来输入如下代码（关键步骤），把github上面的仓库克隆到本地git clone https://github.com/BlueWindQAQ/blog-hexo.git（https://github.com/BlueWindQAQ/blog-hexo.git替换成你之前复制的地址） 这个步骤以后你的本地项目文件夹下面就会多出个文件夹，该文件夹名即为你github上面的项目名，我们把本地项目文件夹下的所有文件（除了新多出的那个文件夹不用），其余都复制到那个新多出的文件夹下， 接下来依次输入以下代码即可完成其他剩余操作： git add . （注：别忘记后面的.，此操作是把文件夹下面的文件都添加进来）git commit -m “提交信息” （注：“提交信息”里面换成你需要，如“first commit”）git push -u origin master （注：此操作目的是把本地仓库push到github上面，此步骤需要你输入帐号和密码） 原文：https://www.cnblogs.com/cxk1995/p/5800196.html]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[针对innodb_flush_method参数的理解和对比测试（mycat+mysql）]]></title>
    <url>%2F2018%2F08%2F30%2Fmysql-innodb-flush-method%2F</url>
    <content type="text"><![CDATA[mysql的innodb_flush_method这个参数控制着innodb数据文件及redo log的打开、刷写模式，对于这个参数，文档上是这样描述的：有三个值：fdatasync(默认)，O_DSYNC，O_DIRECT默认是fdatasync，调用fsync()去刷数据文件与redo log的buffer为O_DSYNC时，innodb会使用O_SYNC方式打开和刷写redo log,使用fsync()刷写数据文件为O_DIRECT时，innodb使用O_DIRECT打开数据文件，使用fsync()刷写数据文件跟redo log首先文件的写操作包括三步：open,write,flush上面最常提到的fsync(int fd)函数，该函数作用是flush时将与fd文件描述符所指文件有关的buffer刷写到磁盘，并且flush完元数据信息(比如修改日期、创建日期等)才算flush成功。使用O_DSYNC方式打开redo文件表示当write日志时，数据都write到磁盘，并且元数据也需要更新，才返回成功。O_DIRECT则表示我们的write操作是从MySQL innodb buffer里直接向磁盘上写。 这三种模式写数据方式具体如下： fdatasync模式：写数据时，write这一步并不需要真正写到磁盘才算完成（可能写入到操作系统buffer中就会返回完成），真正完成是flush操作，buffer交给操作系统去flush,并且文件的元数据信息也都需要更新到磁盘。O_DSYNC模式：写日志操作是在write这步完成，而数据文件的写入是在flush这步通过fsync完成O_DIRECT模式：数据文件的写入操作是直接从mysql innodb buffer到磁盘的，并不用通过操作系统的缓冲，而真正的完成也是在flush这步,日志还是要经过OS缓冲 原文：https://blog.csdn.net/smooth00/article/details/72725941]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iostat数据中关于rKB/s 和wKB/s 列解读]]></title>
    <url>%2F2018%2F08%2F29%2Flinux-iostat%2F</url>
    <content type="text"><![CDATA[导引rKB/s 和wKB/s这两个参数,究竟怎么样才算是读写高1234567891011avg-cpu: %user %nice %system %iowait %steal %idle 36.04 0.00 14.47 1.78 0.00 47.72Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilvda 0.00 0.00 125.00 278.00 2000.00 11617.00 67.58 0.55 1.36 1.07 1.49 0.77 31.20avg-cpu: %user %nice %system %iowait %steal %idle 23.65 0.00 7.46 1.29 0.00 67.61Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilvda 0.00 0.00 71.00 271.00 1136.00 11287.00 72.65 0.53 1.55 1.17 1.65 0.77 26.40 含义: 而rKB/s 和wKB/s 列:每秒千字节为单位显示了读和写的数据量 如果这两对数据值都很高的话说明磁盘io操作是很频繁(注:高是怎么衡量的?) 两对数据值都很高的话说明磁盘io操作是很频繁]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>I/O</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webstorm 2018 激活破解方法]]></title>
    <url>%2F2018%2F08%2F29%2Fwebstorm-pojie%2F</url>
    <content type="text"><![CDATA[webstorm 作为最近最火的前端开发工具,也确实对得起那个价格,但是秉着勤俭节约的传统美德,我们肯定是能省则省啊。方法一注册时，在打开的License Activation窗口中选择“License server”，在输入框输入下面的网址：http://180.76.140.202:9123 (2018/07/20)http://idea.wrbugtest.tk/ (2018/06/16)点击：Activate即可。更多方法移步原文：https://blog.csdn.net/voke_/article/details/76418116]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>webstorm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crontab命令]]></title>
    <url>%2F2018%2F08%2F29%2Flinux-crontab%2F</url>
    <content type="text"><![CDATA[crontab命令被用来提交和管理用户的需要周期性执行的任务，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。 语法1crontab(选项)(参数) 选项1234-e：编辑该用户的计时器设置；-l：列出该用户的计时器设置；-r：删除该用户的计时器设置；-u&lt;用户名称&gt;：指定要设定计时器的用户名称。 参数crontab文件：指定包含待执行任务的crontab文件。 知识扩展Linux下的任务调度分为两类：系统任务调度和用户任务调度。 系统任务调度：系统周期性所要执行的工作，比如写缓存数据到硬盘、日志清理等。在/etc目录下有一个crontab文件，这个就是系统任务调度的配置文件。 /etc/crontab文件包括下面几行：123456789SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=""HOME=/# run-parts51 * * * * root run-parts /etc/cron.hourly24 7 * * * root run-parts /etc/cron.daily22 4 * * 0 root run-parts /etc/cron.weekly42 4 1 * * root run-parts /etc/cron.monthly 前四行是用来配置crond任务运行的环境变量，第一行SHELL变量指定了系统要使用哪个shell，这里是bash，第二行PATH变量指定了系统执行命令的路径，第三行MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户，第四行的HOME变量指定了在执行命令或者脚本时使用的主目录。 用户任务调度：用户定期要执行的工作，比如用户数据备份、定时邮件提醒等。用户可以使用 crontab 工具来定制自己的计划任务。所有用户定义的crontab文件都被保存在/var/spool/cron目录中。其文件名与用户名一致，使用者权限文件如下：123/etc/cron.deny 该文件中所列用户不允许使用crontab命令/etc/cron.allow 该文件中所列用户允许使用crontab命令/var/spool/cron/ 所有用户crontab文件存放的目录,以用户名命名 crontab文件的含义：用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下：1minute hour day month week command 顺序：分 时 日 月 周 其中： minute： 表示分钟，可以是从0到59之间的任何整数。 hour：表示小时，可以是从0到23之间的任何整数。 day：表示日期，可以是从1到31之间的任何整数。 month：表示月份，可以是从1到12之间的任何整数。 week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。 command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。在以上各个字段中，还可以使用以下特殊字符： 星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” 中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” 正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。 crond服务1234/sbin/service crond start //启动服务/sbin/service crond stop //关闭服务/sbin/service crond restart //重启服务/sbin/service crond reload //重新载入配置 查看crontab服务状态：1service crond status 手动启动crontab服务：1service crond start 查看crontab服务是否已设置为开机启动，执行命令：1ntsysv 加入开机自动启动：1chkconfig –level 35 crond on 实例每1分钟执行一次command1* * * * * command 每小时的第3和第15分钟执行13,15 * * * * command 在上午8点到11点的第3和第15分钟执行13,15 8-11 * * * command 每隔两天的上午8点到11点的第3和第15分钟执行13,15 8-11 */2 * * command 每个星期一的上午8点到11点的第3和第15分钟执行13,15 8-11 * * 1 command 每晚的21:30重启smb130 21 * * * /etc/init.d/smb restart 每月1、10、22日的4 : 45重启smb145 4 1,10,22 * * /etc/init.d/smb restart 每周六、周日的1:10重启smb110 1 * * 6,0 /etc/init.d/smb restart 每天18 : 00至23 : 00之间每隔30分钟重启smb10,30 18-23 * * * /etc/init.d/smb restart 每星期六的晚上11:00 pm重启smb10 23 * * 6 /etc/init.d/smb restart 每一小时重启smb1* */1 * * * /etc/init.d/smb restart 晚上11点到早上7点之间，每隔一小时重启smb1* 23-7/1 * * * /etc/init.d/smb restart 每月的4号与每周一到周三的11点重启smb10 11 4 * mon-wed /etc/init.d/smb restart 一月一号的4点重启smb10 4 1 jan * /etc/init.d/smb restart 每小时执行/etc/cron.hourly目录内的脚本101 * * * * root run-parts /etc/cron.hourly 原文：http://man.linuxde.net/crontab]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>crontab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis哨兵模式]]></title>
    <url>%2F2018%2F08%2F28%2Fredis-sentinel%2F</url>
    <content type="text"><![CDATA[Redis Sentinel 模式简介Redis-Sentinel是官方推荐的高可用解决方案，当redis在做master-slave的高可用方案时，假如master宕机了，redis本身（以及其很多客户端）都没有实现自动进行主备切换，而redis-sentinel本身也是独立运行的进程，可以部署在其他与redis集群可通讯的机器中监控redis集群。 它的主要功能有一下几点1、不时地监控redis是否按照预期良好地运行;2、如果发现某个redis节点运行出现状况，能够通知另外一个进程(例如它的客户端);3、能够进行自动切换。当一个master节点不可用时，能够选举出master的多个slave(如果有超过一个slave的话)中的一个来作为新的master,其它的slave节点会将它所追随的master的地址改为被提升为master的slave的新地址。4、哨兵为客户端提供服务发现，客户端链接哨兵，哨兵提供当前master的地址然后提供服务，如果出现切换，也就是master挂了，哨兵会提供客户端一个新地址。 哨兵（sentinel）本身也是支持集群的很显然，单个哨兵会存在自己挂掉而无法监控整个集群的问题，所以哨兵也是支持集群的，我们通常用三台哨兵机器来监控一组redis集群。 快速开始！环境准备：centos7服务器3台,6也可以，没什么区别。我部署好了三台redis-1redis-2redis-3清空selinux与iptables 编译安装redis12345678910111213141516171819yum install gcc* tcl -ymkdir /opt/softcd /opt/softwget http://download.redis.io/releases/redis-3.2.4.tar.gztar xf redis-3.2.4.tar.gzcd redis-3.2.4makemkdir confmkdir bincp utils/redis_init_script bin/rediscp redis.conf conf/6379.confcd ..mv redis-3.2.4 /opt/rediscd /opt/redis/binsed -i s#CLIEXEC=\/usr\/local\/bin\/redis-cli#CLIEXEC=\/opt\/redis\/src\/redis-cli#g redissed -i s#EXEC=\/usr\/local\/bin\/redis-server#EXEC=\/opt\/redis\/src\/redis-server#g redissed -i s#CONF=\"\/etc\/redis#CONF=\"\/opt\/redis\/conf#g rediscd /opt/redis/confsed -i s#daemonize\ no#daemonize\ yes#g 6379.conf 安装完毕后，修改配置文件。vim /opt/redis/conf/6379.conf注释此条bind 127.0.0.1protected-mode yes 改为 protected-mode no #关闭安全模式至此，redis部署完毕。 redis的启动停止脚本在/opt/redis/bin/redis stop/startredis的配置文件在/opt/redis/conf/6379.confredis的登陆命令在/opt/redis/src/redis-cli redis配置主从启动两台redisredis-1 10.0.0.10redis-2 10.0.0.20 若redis-1为主的话，在redis-2的配置文件中配置slaveof 10.0.0.10 6379修改完毕后重启redis即可，重启后我们可通过登陆进入redis后info查看主从信息。 引入哨兵。redis-1与redis-2搭建完毕主从后，我们开始引入哨兵。哨兵是一个单独的程序，所以我们需要单独部署它。若是在其他机器上部署哨兵，那么请用上面的redis安装脚本重新安装一遍redis。在这里我已经部署完毕了redis-1redis-2redis-3 增加哨兵的配置文件。三台redis都需要增加，文件内容这三台一样。123456789vim /opt/redis/conf/sentinel.conf##sentinel for 10.0.0.10 ,its slave is 10.0.0.20#master1port 26383sentinel monitor master1 10.0.0.10 6379 2sentinel down-after-milliseconds master1 30000sentinel failover-timeout master1 900000sentinel parallel-syncs master1 1#sentinel auth-pass mymaster 123456 #如果你的redis集群有密码 配置文件的含义请自行百度。 启动哨兵三台机器都是一个操作方式。 添加窗口screen -S sentinel 在新窗口启动哨兵/opt/redis/src/redis-sentinel /opt/redis/conf/sentinel.conf –protected-mode no启动后即可看到前台输出信息。 后台挂起这个窗口请按：Ctrl+a+d 下次返回观看这个窗口请输入screen -r sentinel 我们这里暂时不挂起窗口，可以观察哨兵监控集群的状态。 我们接下来我们进行切换以及增加新的salve节点测试。关掉redis-1并查看哨兵监控的状态，约30秒内，哨兵探测redis-1客观故障后，即会重新选举新的master，重新选举完毕后我们在redis-2中info查看主从状态，会发现redis-2已经被选举为master。重新启动redis-1，并不需要修改配置文件，启动后redis-1自动会与redis-2进行同步。 修改redis-3的配置文件，把slaveof指向到redis-2，启动后你会发现哨兵会把redis-3自动添加到集群中。 原文：https://www.cnblogs.com/kerwinC/p/6069864.html]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo NexT主题中集成gitalk评论系统]]></title>
    <url>%2F2018%2F08%2F23%2Fhexo-gitalk%2F</url>
    <content type="text"><![CDATA[记录在NexT主题中添加gitalk评论系统的步骤。gitalk：一个基于 Github Issue 和 Preact 开发的评论插件详情Demo可见：https://gitalk.github.io/ Register Application在GitHub上注册新应用，链接：https://github.com/settings/applications/new参数说明：Application name： # 应用名称，随意Homepage URL： # 网站URL，如https://asdfv1929.github.ioApplication description # 描述，随意Authorization callback URL：# 网站URL，https://asdfv1929.github.io 点击注册后，页面跳转如下，其中Client ID和Client Secret在后面的配置中需要用到，到时复制粘贴即可： gitalk.swig新建/layout/_third-party/comments/gitalk.swig文件，并添加内容：12345678910111213141516&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"&gt; &lt;script src="https://unpkg.com/gitalk/dist/gitalk.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; var gitalk = new Gitalk(&#123; clientID: '&#123;&#123; theme.gitalk.ClientID &#125;&#125;', clientSecret: '&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;', repo: '&#123;&#123; theme.gitalk.repo &#125;&#125;', owner: '&#123;&#123; theme.gitalk.githubID &#125;&#125;', admin: ['&#123;&#123; theme.gitalk.adminUser &#125;&#125;'], id: location.pathname, distractionFreeMode: '&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;' &#125;) gitalk.render('gitalk-container') &lt;/script&gt;&#123;% endif %&#125; comments.swig修改/layout/_partials/comments.swig，添加内容如下，与前面的elseif同一级别上：12&#123;% elseif theme.gitalk.enable %&#125; &lt;div id="gitalk-container"&gt;&lt;/div&gt; index.swig修改layout/_third-party/comments/index.swig，在最后一行添加内容：1&#123;% include 'gitalk.swig' %&#125; gitalk.styl新建/source/css/_common/components/third-party/gitalk.styl文件，添加内容：1234.gt-header a, .gt-comments a, .gt-popup a border-bottom: none;.gt-container .gt-popup .gt-action.is--active:before top: 0.7em; third-party.styl修改/source/css/_common/components/third-party/third-party.styl，在最后一行上添加内容，引入样式：1@import "gitalk"; _config.yml在主题配置文件next/_config.yml中添加如下内容：12345678gitalk: enable: true githubID: github帐号 # 例：asdfv1929 repo: 仓库名称 # 例：asdfv1929.github.io ClientID: Client ID ClientSecret: Client Secret adminUser: github帐号 #指定可初始化评论账户 distractionFreeMode: true 以上就是NexT中添加gitalk评论的配置，博客上传到GitHub上后，打开页面进入某一博客内容下，就可看到评论处。 部分问题解决方法，可参见：https://liujunzhou.cn/2018/8/10/gitalk-error/#more 原文：https://asdfv1929.github.io/2018/01/20/gitalk]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo添加右侧公众号]]></title>
    <url>%2F2018%2F08%2F23%2Fhexo-wechat-qrcode%2F</url>
    <content type="text"><![CDATA[hexo添加右侧公众号页面需在两处添加代码1、csscss添加在主题文件下的source/css/_schemes/主题名称文件夹/_layout123456789101112131415161718192021222324252627282930313233343536373839.paral&#123; position: absolute; top: 10%; right: 0; width:200px; height:140px; -webkit-user-select:none; ##禁止鼠标选中 -moz-user-select:none; -ms-user-select:none; user-select:none;&#125;.zi&#123; position: absolute; right: 0; width:30px; height:120px; writing-mode: vertical-lr; ##文字垂直 text-align:center; ##文字居中 background-color:#e2e5b7; font-size:18px; cursor:pointer; ##划过时鼠标样式 top:50%; transform:translateY(-50%); ##p标签垂直居中 border:1px solid #d1d8bd; border-radius:5px; ##边框圆角&#125;.img&#123; position: absolute; right: 60px; width: 140px; height: 140px; display: none; ##默认不显示图片&#125;p:hover +.img &#123; display: block; ##鼠标划过时，改变class--img属性为显示&#125; 2、HTMLHTML代码添加到主题文件夹下layout/_layout.swig中class为main中1234&lt;div class="paral"&gt; &lt;p class="zi"&gt;公众号&lt;/p&gt; &lt;img src="/uploads/qrcode_for_smartfoot.jpg" class="img"&gt;&lt;/div&gt;]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM大对象直接进入老年代]]></title>
    <url>%2F2018%2F08%2F20%2Fa8%2F</url>
    <content type="text"><![CDATA[原文：https://book.2cto.com/201306/25496.html 虚拟机提供了一个-XX:PretenureSizeThreshold参数，令大于这个设置值的对象直接在老年代分配。这样做的目的是避免在Eden区及两个Survivor区之间发生大量的内存复制（复习一下：新生代采用复制算法收集内存）。 所谓的大对象是指，需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串以及数组（笔者列出的例子中的byte[]数组就是典型的大对象）。大对象对虚拟机的内存分配来说就是一个坏消息（替Java虚拟机抱怨一句，比遇到一个大对象更加坏的消息就是遇到一群“朝生夕灭”的“短命大对象”，写程序的时候应当避免），经常出现大对象容易导致内存还有不少空间时就提前触发垃圾收集以获取足够的连续空间来“安置”它们。 注意 PretenureSizeThreshold参数只对Serial和ParNew两款收集器有效，Parallel Scavenge收集器不认识这个参数，Parallel Scavenge收集器一般并不需要设置。如果遇到必须使用此参数的场合，可以考虑ParNew加CMS的收集器组合。]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM出现频繁GC (Allocation Failure)及young gc时间过长分析]]></title>
    <url>%2F2018%2F08%2F20%2Fa7%2F</url>
    <content type="text"><![CDATA[序本文主要分析一个频繁GC (Allocation Failure)及young gc时间过长的case。 症状 gc throughput percent逐步下降，从一般的99.96%逐步下降，跌破99%，进入98%，最低点能到94% young gc time逐步增加，从一般的十几毫秒逐步上升，突破50，再突破100，150，200，250 在8.5天的时间内，发生了9000多次gc，其中full gc为4次，平均将近8秒，大部分是young gc(allocation failure为主)，平均270多毫秒，最大值将近7秒 平均对象创建速率为10.63 mb/sec，平均的晋升速率为2 kb/sec，cpu使用率正常，没有明显的飙升jvm参数https://my.oschina.net/go4it/blog/1628795]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM -XX:NewRatio、-XX:SurvivorRatio参数含义]]></title>
    <url>%2F2018%2F08%2F20%2Fa6%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高并发下载tomcat下的文件时,发生java.net.SocketException:Connection reset解决方案]]></title>
    <url>%2F2018%2F08%2F20%2Fa5%2F</url>
    <content type="text"><![CDATA[可能是服务器连接超过最大并发数而重置，导致客户端连接超时在tomcat的conf目录下，查看server.xml找到&lt;Connector port=”8080” 标签内添加，确认下当前的连接，可以做如下调整（具体需要根据实际情况设定）：12maxThreads="500" minSpareThreads="50" maxSpareThreads="100" enableLookups="false" acceptCount="500" 参考1、http://blog.sina.com.cn/s/blog_43eb83b90102ds8w.html2、http://www.cnblogs.com/qqzy168/archive/2012/09/04/2670002.html]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springmvc + mybatis 遍历Map]]></title>
    <url>%2F2018%2F08%2F20%2Fa4%2F</url>
    <content type="text"><![CDATA[spring的application.xml中数据源:12345678910111213&lt;!-- enterpriseUser数据库 --&gt;&lt;bean id="enterpriseUserDataSource" class="org.apache.commons.dbcp2.BasicDataSource"&gt; &lt;property name="driverClassName" value="$&#123;td.jdbc.enterpriseuser.driverClassName&#125;"/&gt; &lt;property name="url" value="$&#123;td.jdbc.enterpriseuser.url&#125;"/&gt; &lt;property name="username" value="$&#123;td.jdbc.enterpriseuser.username&#125;"/&gt; &lt;property name="password" value="$&#123;td.jdbc.enterpriseuser.password&#125;"/&gt; &lt;property name="initialSize" value="$&#123;td.jdbc.enterpriseuser.initialSize&#125;"/&gt; &lt;property name="maxTotal" value="$&#123;td.jdbc.enterpriseuser.maxActive&#125;"/&gt; &lt;property name="maxIdle" value="$&#123;td.jdbc.enterpriseuser.maxIdle&#125;"/&gt; &lt;property name="minIdle" value="$&#123;td.jdbc.enterpriseuser.minIdle&#125;"/&gt; &lt;property name="testOnBorrow" value="$&#123;td.jdbc.enterpriseuser.testOnBorrow&#125;"/&gt; &lt;property name="validationQuery" value="$&#123;td.jdbc.enterpriseuser.validationQuery&#125;"/&gt;&lt;/bean&gt; sessionFactory:12345678bean id="enterpriseSqlFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;!-- 注入数据库连接池 --&gt; &lt;property name="dataSource" ref="enterpriseDataSource" /&gt; &lt;!-- 配置MyBaties全局配置文件:mybatis-config.xml --&gt; &lt;property name="configLocation" value="classpath:mybatis/mybatis-config.xml" /&gt; &lt;!-- 扫描sql配置文件:mapper需要的xml文件 --&gt; &lt;property name="mapperLocations" value="classpath:mybatis/enterprise_mapper/*.xml" /&gt;&lt;/bean&gt; Mapper.xml中的一个查询:123456789101112131415&lt;select id="queryBitmapIsolateParams" resultType="java.util.Map" parameterType="java.util.Map"&gt; &lt;!-- 具体的sql --&gt; $&#123;sql&#125; &lt;!-- 具体参数 --&gt; &lt;where&gt; &lt;if test="params != null"&gt; &lt;foreach collection="params.keys" item="key" separator="and"&gt; &lt;choose&gt; &lt;when test="#&#123;params[#&#123;key&#125;]&#125; == null"&gt; `$&#123;key&#125;` IS NULL &lt;/when&gt; &lt;otherwise&gt; `$&#123;key&#125;` = #&#123;params[$&#123;key&#125;]&#125; &lt;/otherwise&gt; &lt;/choose&gt; &lt;/foreach&gt; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt;]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>springmvc</tag>
        <tag>mybatis</tag>
        <tag>遍历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven settings.xml详解]]></title>
    <url>%2F2018%2F08%2F15%2Fa3%2F</url>
    <content type="text"><![CDATA[setting.xml配置文件maven的配置文件settings.xml存在于两个地方： 1.安装的地方：${M2_HOME}/conf/settings.xml 2.用户的目录：${user.home}/.m2/settings.xml 前者又被叫做全局配置，对操作系统的所有使用者生效；后者被称为用户配置，只对当前操作系统的使用者生效。如果两者都存在，它们的内容将被合并，并且用户范围的settings.xml会覆盖全局的settings.xml。Maven安装后，用户目录下不会自动生成settings.xml，只有全局配置文件。如果需要创建用户范围的settings.xml，可以将安装路径下的settings复制到目录${user.home}/.m2/。Maven默认的settings.xml是一个包含了注释和例子的模板，可以快速的修改它来达到你的要求。 全局配置一旦更改，所有的用户都会受到影响，而且如果maven进行升级，所有的配置都会被清除，所以要提前复制和备份${M2_HOME}/conf/settings.xml文件，一般情况下不推荐配置全局的settings.xml。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;settings xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; &lt;!--本地仓库。该值表示构建系统本地仓库的路径。其默认值为$&#123;user.home&#125;/.m2/repository。 --&gt; &lt;localRepository&gt;usr/local/maven&lt;/localRepository&gt; &lt;!--Maven是否需要和用户交互以获得输入。如果Maven需要和用户交互以获得输入，则设置成true，反之则应为false。默认为true。 --&gt; &lt;interactiveMode&gt;true&lt;/interactiveMode&gt; &lt;!--Maven是否需要使用plugin-registry.xml文件来管理插件版本。 --&gt; &lt;!--如果设置为true，则在&#123;user.home&#125;/.m2下需要有一个plugin-registry.xml来对plugin的版本进行管理 --&gt; &lt;!--默认为false。 --&gt; &lt;usePluginRegistry&gt;false&lt;/usePluginRegistry&gt; &lt;!--表示Maven是否需要在离线模式下运行。如果构建系统需要在离线模式下运行，则为true，默认为false。 --&gt; &lt;!--当由于网络设置原因或者安全因素，构建服务器不能连接远程仓库的时候，该配置就十分有用。 --&gt; &lt;offline&gt;false&lt;/offline&gt; &lt;!--当插件的组织Id（groupId）没有显式提供时，供搜寻插件组织Id（groupId）的列表。 --&gt; &lt;!--该元素包含一个pluginGroup元素列表，每个子元素包含了一个组织Id（groupId）。 --&gt; &lt;!--当我们使用某个插件，并且没有在命令行为其提供组织Id（groupId）的时候，Maven就会使用该列表。 --&gt; &lt;!--默认情况下该列表包含了org.apache.maven.plugins。 --&gt; &lt;pluginGroups&gt; &lt;!--plugin的组织Id（groupId） --&gt; &lt;pluginGroup&gt;org.codehaus.mojo&lt;/pluginGroup&gt; &lt;/pluginGroups&gt; &lt;!--用来配置不同的代理，多代理profiles可以应对笔记本或移动设备的工作环境：通过简单的设置profile id就可以很容易的更换整个代理配置。 --&gt; &lt;proxies&gt; &lt;!--代理元素包含配置代理时需要的信息 --&gt; &lt;proxy&gt; &lt;!--代理的唯一定义符，用来区分不同的代理元素。 --&gt; &lt;id&gt;myproxy&lt;/id&gt; &lt;!--该代理是否是激活的那个。true则激活代理。当我们声明了一组代理，而某个时候只需要激活一个代理的时候，该元素就可以派上用处。 --&gt; &lt;active&gt;true&lt;/active&gt; &lt;!--代理的协议。 协议://主机名:端口，分隔成离散的元素以方便配置。 --&gt; &lt;protocol&gt;http://…&lt;/protocol&gt; &lt;!--代理的主机名。协议://主机名:端口，分隔成离散的元素以方便配置。 --&gt; &lt;host&gt;proxy.somewhere.com&lt;/host&gt; &lt;!--代理的端口。协议://主机名:端口，分隔成离散的元素以方便配置。 --&gt; &lt;port&gt;8080&lt;/port&gt; &lt;!--代理的用户名，用户名和密码表示代理服务器认证的登录名和密码。 --&gt; &lt;username&gt;proxyuser&lt;/username&gt; &lt;!--代理的密码，用户名和密码表示代理服务器认证的登录名和密码。 --&gt; &lt;password&gt;somepassword&lt;/password&gt; &lt;!--不该被代理的主机名列表。该列表的分隔符由代理服务器指定；例子中使用了竖线分隔符，使用逗号分隔也很常见。 --&gt; &lt;nonProxyHosts&gt;*.google.com|ibiblio.org&lt;/nonProxyHosts&gt; &lt;/proxy&gt; &lt;/proxies&gt; &lt;!--配置服务端的一些设置。一些设置如安全证书不应该和pom.xml一起分发。这种类型的信息应该存在于构建服务器上的settings.xml文件中。 --&gt; &lt;servers&gt; &lt;!--服务器元素包含配置服务器时需要的信息 --&gt; &lt;server&gt; &lt;!--这是server的id（注意不是用户登陆的id），该id与distributionManagement中repository元素的id相匹配。 --&gt; &lt;id&gt;server001&lt;/id&gt; &lt;!--鉴权用户名。鉴权用户名和鉴权密码表示服务器认证所需要的登录名和密码。 --&gt; &lt;username&gt;my_login&lt;/username&gt; &lt;!--鉴权密码 。鉴权用户名和鉴权密码表示服务器认证所需要的登录名和密码。 --&gt; &lt;password&gt;my_password&lt;/password&gt; &lt;!--鉴权时使用的私钥位置。和前两个元素类似，私钥位置和私钥密码指定了一个私钥的路径（默认是/home/hudson/.ssh/id_dsa）以及如果需要的话，一个密钥 --&gt; &lt;!--将来passphrase和password元素可能会被提取到外部，但目前它们必须在settings.xml文件以纯文本的形式声明。 --&gt; &lt;privateKey&gt;$&#123;usr.home&#125;/.ssh/id_dsa&lt;/privateKey&gt; &lt;!--鉴权时使用的私钥密码。 --&gt; &lt;passphrase&gt;some_passphrase&lt;/passphrase&gt; &lt;!--文件被创建时的权限。如果在部署的时候会创建一个仓库文件或者目录，这时候就可以使用权限（permission）。--&gt; &lt;!--这两个元素合法的值是一个三位数字，其对应了unix文件系统的权限，如664，或者775。 --&gt; &lt;filePermissions&gt;664&lt;/filePermissions&gt; &lt;!--目录被创建时的权限。 --&gt; &lt;directoryPermissions&gt;775&lt;/directoryPermissions&gt; &lt;!--传输层额外的配置项 --&gt; &lt;configuration&gt;&lt;/configuration&gt; &lt;/server&gt; &lt;/servers&gt; &lt;!--为仓库列表配置的下载镜像列表。 --&gt; &lt;mirrors&gt; &lt;!--给定仓库的下载镜像。 --&gt; &lt;mirror&gt; &lt;!--该镜像的唯一标识符。id用来区分不同的mirror元素。 --&gt; &lt;id&gt;planetmirror.com&lt;/id&gt; &lt;!--镜像名称 --&gt; &lt;name&gt;PlanetMirror Australia&lt;/name&gt; &lt;!--该镜像的URL。构建系统会优先考虑使用该URL，而非使用默认的服务器URL。 --&gt; &lt;url&gt;http://downloads.planetmirror.com/pub/maven2&lt;/url&gt; &lt;!--被镜像的服务器的id。例如，如果我们要设置了一个Maven中央仓库（http://repo1.maven.org/maven2）的镜像，--&gt; &lt;!--就需要将该元素设置成central。这必须和中央仓库的id central完全一致。 --&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;!--根据环境参数来调整构建配置的列表。settings.xml中的profile元素是pom.xml中profile元素的裁剪版本。--&gt; &lt;!--它包含了id，activation, repositories, pluginRepositories和 properties元素。--&gt; &lt;!--这里的profile元素只包含这五个子元素是因为这里只关心构建系统这个整体（这正是settings.xml文件的角色定位），而非单独的项目对象模型设置。--&gt; &lt;!--如果一个settings中的profile被激活，它的值会覆盖任何其它定义在POM中或者profile.xml中的带有相同id的profile。 --&gt; &lt;profiles&gt; &lt;!--根据环境参数来调整的构件的配置 --&gt; &lt;profile&gt; &lt;!--该配置的唯一标识符。 --&gt; &lt;id&gt;test&lt;/id&gt; &lt;!--自动触发profile的条件逻辑。Activation是profile的开启钥匙。--&gt; &lt;!--如POM中的profile一样，profile的力量来自于它能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。--&gt; &lt;!--activation元素并不是激活profile的唯一方式。settings.xml文件中的activeProfile元素可以包含profile的id。--&gt; &lt;!--profile也可以通过在命令行，使用-P标记和逗号分隔的列表来显式的激活（如，-P test）。 --&gt; &lt;activation&gt; &lt;!--profile默认是否激活的标识 --&gt; &lt;activeByDefault&gt;false&lt;/activeByDefault&gt; &lt;!--activation有一个内建的java版本检测，如果检测到jdk版本与期待的一样，profile被激活。 --&gt; &lt;jdk&gt;1.7&lt;/jdk&gt; &lt;!--当匹配的操作系统属性被检测到，profile被激活。os元素可以定义一些操作系统相关的属性。 --&gt; &lt;os&gt; &lt;!--激活profile的操作系统的名字 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!--激活profile的操作系统所属家族(如 'windows') --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!--激活profile的操作系统体系结构 --&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;!--激活profile的操作系统版本 --&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;!--如果Maven检测到某一个属性（其值可以在POM中通过$&#123;名称&#125;引用），其拥有对应的名称和值，Profile就会被激活。--&gt; &lt;!--如果值字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段 --&gt; &lt;property&gt; &lt;!--激活profile的属性的名称 --&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!--激活profile的属性的值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!--提供一个文件名，通过检测该文件的存在或不存在来激活profile。missing检查文件是否存在，如果不存在则激活profile。--&gt; &lt;!--另一方面，exists则会检查文件是否存在，如果存在则激活profile。 --&gt; &lt;file&gt; &lt;!--如果指定的文件存在，则激活profile。 --&gt; &lt;exists&gt;/usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/&lt;/exists&gt; &lt;!--如果指定的文件不存在，则激活profile。 --&gt; &lt;missing&gt;/usr/local/hudson/hudson-home/jobs/maven-guide-zh-to-production/workspace/&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; &lt;!--对应profile的扩展属性列表。Maven属性和Ant中的属性一样，可以用来存放一些值。这些值可以在POM中的任何地方使用标记$&#123;X&#125;来使用，这里X是指属性的名称。--&gt; &lt;!--属性有五种不同的形式，并且都能在settings.xml文件中访问。 --&gt; &lt;!--1. env.X: 在一个变量前加上"env."的前缀，会返回一个shell环境变量。例如,"env.PATH"指代了$path环境变量（在Windows上是%PATH%）。 --&gt; &lt;!--2. project.x：指代了POM中对应的元素值。 --&gt; &lt;!--3. settings.x: 指代了settings.xml中对应元素的值。 --&gt; &lt;!--4. Java System Properties: 所有可通过java.lang.System.getProperties()访问的属性都能在POM中使用该形式访问， --&gt; &lt;!-- 如/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0/jre。 --&gt; &lt;!--5. x: 在&lt;properties/&gt;元素中，或者外部文件中设置，以$&#123;someVar&#125;的形式使用。 --&gt; &lt;properties&gt; &lt;!-- 如果这个profile被激活，那么属性$&#123;user.install&#125;就可以被访问了 --&gt; &lt;user.install&gt;usr/local/winner/jobs/maven-guide&lt;/user.install&gt; &lt;/properties&gt; &lt;!--远程仓库列表，它是Maven用来填充构建系统本地仓库所使用的一组远程项目。 --&gt; &lt;repositories&gt; &lt;!--包含需要连接到远程仓库的信息 --&gt; &lt;repository&gt; &lt;!--远程仓库唯一标识 --&gt; &lt;id&gt;codehausSnapshots&lt;/id&gt; &lt;!--远程仓库名称 --&gt; &lt;name&gt;Codehaus Snapshots&lt;/name&gt; &lt;!--如何处理远程仓库里发布版本的下载 --&gt; &lt;releases&gt; &lt;!--true或者false表示该仓库是否为下载某种类型构件（发布版，快照版）开启。 --&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;!--该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。这里的选项是：--&gt; &lt;!--always（一直），daily（默认，每日），interval：X（这里X是以分钟为单位的时间间隔），或者never（从不）。 --&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;!--当Maven验证构件校验文件失败时该怎么做:--&gt; &lt;!--ignore（忽略），fail（失败），或者warn（警告）。 --&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/releases&gt; &lt;!--如何处理远程仓库里快照版本的下载。有了releases和snapshots这两组配置，POM就可以在每个单独的仓库中，为每种类型的构件采取不同的策略。--&gt; &lt;!--例如，可能有人会决定只为开发目的开启对快照版本下载的支持。参见repositories/repository/releases元素 --&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;!--远程仓库URL，按protocol://hostname/path形式 --&gt; &lt;url&gt;http://snapshots.maven.codehaus.org/maven2&lt;/url&gt; &lt;!--用于定位和排序构件的仓库布局类型-可以是default（默认）或者legacy（遗留）。--&gt; &lt;!--Maven 2为其仓库提供了一个默认的布局；然而，Maven 1.x有一种不同的布局。我们可以使用该元素指定布局是default（默认）还是legacy（遗留）。 --&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;!--发现插件的远程仓库列表。仓库是两种主要构件的家。第一种构件被用作其它构件的依赖。这是中央仓库中存储的大部分构件类型。另外一种构件类型是插件。--&gt; &lt;!--Maven插件是一种特殊类型的构件。由于这个原因，插件仓库独立于其它仓库。pluginRepositories元素的结构和repositories元素的结构类似。--&gt; &lt;!--每个pluginRepository元素指定一个Maven可以用来寻找新插件的远程地址。 --&gt; &lt;pluginRepositories&gt; &lt;!--包含需要连接到远程插件仓库的信息.参见profiles/profile/repositories/repository元素的说明 --&gt; &lt;pluginRepository&gt; &lt;releases&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;id /&gt; &lt;name /&gt; &lt;url /&gt; &lt;layout /&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;!--手动激活profiles的列表，按照profile被应用的顺序定义activeProfile。 该元素包含了一组activeProfile元素，每个activeProfile都含有一个profile id。--&gt; &lt;!--任何在activeProfile中定义的profile id，不论环境设置如何，其对应的 profile都会被激活。--&gt; &lt;!--如果没有匹配的profile，则什么都不会发生。例如，env-test是一个activeProfile，则在pom.xml（或者profile.xml）中对应id的profile会被激活。--&gt; &lt;!--如果运行过程中找不到这样一个profile，Maven则会像往常一样运行。 --&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;env-test&lt;/activeProfile&gt; &lt;/activeProfiles&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;/settings&gt; 上面的配置文件对各个节点的含义及作用都有注解。实际应用中，经常使用的是、、、有限几个节点，其他节点使用默认值足够应对大部分的应用场景。 节点在仓库的配置一节中，已经对setting.xml中的常用节点做了详细的说明。在这里需要特别介绍一下的是节点的配置，profile是maven的一个重要特性。 节点包含了激活(activation)，仓库(repositories)，插件仓库(pluginRepositories)和属性(properties)共四个子元素元素。profile元素仅包含这四个元素是因为他们涉及到整个的构建系统，而不是个别的项目级别的POM配置。 profile可以让maven能够自动适应外部的环境变化，比如同一个项目，在linux下编译linux的版本，在win下编译win的版本等。一个项目可以设置多个profile，也可以在同一时间设置多个profile被激活（active）的。自动激活的 profile的条件可以是各种各样的设定条件，组合放置在activation节点中，也可以通过命令行直接指定。如果认为profile设置比较复杂，可以将所有的profiles内容移动到专门的 profiles.xml 文件中，不过记得和pom.xml放在一起。 activation节点是设置该profile在什么条件下会被激活，常见的条件有如下几个： os 判断操作系统相关的参数，它包含如下可以自由组合的子节点元素 message - 规则失败之后显示的消息 arch - 匹配cpu结构，常见为x86 family - 匹配操作系统家族，常见的取值为：dos，mac，netware，os/2，unix，windows，win9x，os/400等 name - 匹配操作系统的名字 version - 匹配的操作系统版本号 display - 检测到操作系统之后显示的信息 jdk 检查jdk版本，可以用区间表示。 property 检查属性值，本节点可以包含name和value两个子节点。 file 检查文件相关内容，包含两个子节点：exists和missing，用于分别检查文件存在和不存在两种情况。 如果settings中的profile被激活，那么它的值将覆盖POM或者profiles.xml中的任何相等ID的profiles。 如果想要某个profile默认处于激活状态，可以在中将该profile的id放进去。这样，不论环境设置如何，其对应的 profile都会被激活。 原文出处：http://blog.csdn.net/u012152619/article/details/51485152]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM初探- 内存分配、GC原理与垃圾收集器]]></title>
    <url>%2F2018%2F08%2F15%2Fa2%2F</url>
    <content type="text"><![CDATA[JVM内存的分配与回收大致可分为如下4个步骤: 何时分配 -&gt; 怎样分配 -&gt; 何时回收 -&gt; 怎样回收. 除了在概念上可简单认为new时分配外, 我们着重介绍后面的3个步骤: I. 怎样分配- JVM内存分配策略对象内存主要分配在新生代Eden区, 如果启用了本地线程分配缓冲, 则优先在TLAB上分配, 少数情况能会直接分配在老年代, 或被拆分成标量类型在栈上分配(JIT优化). 分配的规则并不是百分百固定, 细节主要取决于垃圾收集器组合, 以及VM内存相关的参数. 对象分配 优先在Eden区分配在JVM内存模型一文中, 我们大致了解了VM年轻代堆内存可以划分为一块Eden区和两块Survivor区. 在大多数情况下, 对象在新生代Eden区中分配, 当Eden区没有足够空间分配时, VM发起一次Minor GC, 将Eden区和其中一块Survivor区内尚存活的对象放入另一块Survivor区域, 如果在Minor GC期间发现新生代存活对象无法放入空闲的Survivor区, 则会通过空间分配担保机制使对象提前进入老年代(空间分配担保见下). 大对象直接进入老年代Serial和ParNew两款收集器提供了-XX:PretenureSizeThreshold的参数, 令大于该值的大对象直接在老年代分配, 这样做的目的是避免在Eden区和Survivor区之间产生大量的内存复制(大对象一般指 需要大量连续内存的Java对象, 如很长的字符串和数组), 因此大对象容易导致还有不少空闲内存就提前触发GC以获取足够的连续空间. 对象晋升 年龄阈值VM为每个对象定义了一个对象年龄(Age)计数器, 对象在Eden出生如果经第一次Minor GC后仍然存活, 且能被Survivor容纳的话, 将被移动到Survivor空间中, 并将年龄设为1. 以后对象在Survivor区中每熬过一次Minor GC年龄就+1. 当增加到一定程度(-XX:MaxTenuringThreshold, 默认15), 将会晋升到老年代. 提前晋升: 动态年龄判定然而VM并不总是要求对象的年龄必须达到MaxTenuringThreshold才能晋升老年代: 如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半, 年龄大于或等于该年龄的对象就可以直接进入老年代, 而无须等到晋升年龄. II. 何时回收-对象生死判定(哪些内存需要回收/何时回收) 在堆里面存放着Java世界中几乎所有的对象实例, 垃圾收集器在对堆进行回收前, 第一件事就是判断哪些对象已死(可回收). 可达性分析算法在主流商用语言(如Java、C#)的主流实现中, 都是通过可达性分析算法来判定对象是否存活的: 通过一系列的称为 GC Roots 的对象作为起点, 然后向下搜索; 搜索所走过的路径称为引用链/Reference Chain, 当一个对象到 GC Roots 没有任何引用链相连时, 即该对象不可达, 也就说明此对象是不可用的, 如下图: Object5、6、7 虽然互有关联, 但它们到GC Roots是不可达的, 因此也会被判定为可回收的对象: 在Java, 可作为GC Roots的对象包括: 方法区: 类静态属性引用的对象; 方法区: 常量引用的对象; 虚拟机栈(本地变量表)中引用的对象. 本地方法栈JNI(Native方法)中引用的对象。 注: 即使在可达性分析算法中不可达的对象, VM也并不是马上对其回收, 因为要真正宣告一个对象死亡, 至少要经历两次标记过程: 第一次是在可达性分析后发现没有与GC Roots相连接的引用链, 第二次是GC对在F-Queue执行队列中的对象进行的小规模标记(对象需要覆盖finalize()方法且没被调用过). III. GC原理- 垃圾收集算法分代收集算法 VS 分区收集算法 分代收集当前主流VM垃圾收集都采用”分代收集”(Generational Collection)算法, 这种算法会根据对象存活周期的不同将内存划分为几块, 如JVM中的 新生代、老年代、永久代. 这样就可以根据各年代特点分别采用最适当的GC算法: 在新生代: 每次垃圾收集都能发现大批对象已死, 只有少量存活. 因此选用复制算法, 只需要付出少量存活对象的复制成本就可以完成收集. 在老年代: 因为对象存活率高、没有额外空间对它进行分配担保, 就必须采用“标记—清理”或“标记—整理”算法来进行回收, 不必进行内存复制, 且直接腾出空闲内存. 分区收集上面介绍的分代收集算法是将对象的生命周期按长短划分为两个部分, 而分区算法则将整个堆空间划分为连续的不同小区间, 每个小区间独立使用, 独立回收. 这样做的好处是可以控制一次回收多少个小区间.在相同条件下, 堆空间越大, 一次GC耗时就越长, 从而产生的停顿也越长. 为了更好地控制GC产生的停顿时间, 将一块大的内存区域分割为多个小块, 根据目标停顿时间, 每次合理地回收若干个小区间(而不是整个堆), 从而减少一次GC所产生的停顿. 分代收集新生代-复制算法该算法的核心是将可用内存按容量划分为大小相等的两块, 每次只用其中一块, 当这一块的内存用完, 就将还存活的对象复制到另外一块上面, 然后把已使用过的内存空间一次清理掉. (图片来源: jvm垃圾收集算)这使得每次只对其中一块内存进行回收, 分配也就不用考虑内存碎片等复杂情况, 实现简单且运行高效. 现代商用VM的新生代均采用复制算法, 但由于新生代中的98%的对象都是生存周期极短的, 因此并不需完全按照1∶1的比例划分新生代空间, 而是将新生代划分为一块较大的Eden区和两块较小的Survivor区(HotSpot默认Eden和Survivor的大小比例为8∶1), 每次只用Eden和其中一块Survivor. 当发生MinorGC时, 将Eden和Survivor中还存活着的对象一次性地拷贝到另外一块Survivor上, 最后清理掉Eden和刚才用过的Survivor的空间. 当Survivor空间不够用(不足以保存尚存活的对象)时, 需要依赖老年代进行空间分配担保机制, 这部分内存直接进入老年代. 老年代-标记清除算法该算法分为“标记”和“清除”两个阶段: 首先标记出所有需要回收的对象(可达性分析), 在标记完成后统一清理掉所有被标记的对象. 该算法会有以下两个问题: 效率问题: 标记和清除过程的效率都不高; 空间问题: 标记清除后会产生大量不连续的内存碎片, 空间碎片太多可能会导致在运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集. 老年代-标记整理算法标记清除算法会产生内存碎片问题, 而复制算法需要有额外的内存担保空间, 于是针对老年代的特点, 又有了标记整理算法. 标记整理算法的标记过程与标记清除算法相同, 但后续步骤不再对可回收对象直接清理, 而是让所有存活的对象都向一端移动,然后清理掉端边界以外的内存. 永久代-方法区回收 在方法区进行垃圾回收一般”性价比”较低, 因为在方法区主要回收两部分内容: 废弃常量和无用的类. 回收废弃常量与回收其他年代中的对象类似, 但要判断一个类是否无用则条件相当苛刻: 该类所有的实例都已经被回收, Java堆中不存在该类的任何实例; 该类对应的Class对象没有在任何地方被引用(也就是在任何地方都无法通过反射访问该类的方法); 加载该类的ClassLoader已经被回收.但即使满足以上条件也未必一定会回收, Hotspot VM还提供了-Xnoclassgc参数控制(关闭CLASS的垃圾回收功能). 因此在大量使用动态代理、CGLib等字节码框架的应用中一定要关闭该选项, 开启VM的类卸载功能, 以保证方法区不会溢出. 补充: 空间分配担保在执行Minor GC前, VM会首先检查老年代是否有足够的空间存放新生代尚存活对象, 由于新生代使用复制收集算法, 为了提升内存利用率, 只使用了其中一个Survivor作为轮换备份, 因此当出现大量对象在Minor GC后仍然存活的情况时, 就需要老年代进行分配担保, 让Survivor无法容纳的对象直接进入老年代, 但前提是老年代需要有足够的空间容纳这些存活对象. 但存活对象的大小在实际完成GC前是无法明确知道的, 因此Minor GC前, VM会先首先检查老年代连续空间是否大于新生代对象总大小或历次晋升的平均大小, 如果条件成立, 则进行Minor GC, 否则进行Full GC(让老年代腾出更多空间).然而取历次晋升的对象的平均大小也是有一定风险的, 如果某次Minor GC存活后的对象突增,远远高于平均值的话,依然可能导致担保失败(Handle Promotion Failure, 老年代也无法存放这些对象了), 此时就只好在失败后重新发起一次Full GC(让老年代腾出更多空间). IX. GC实现- 垃圾收集器 GC实现目标: 准确、高效、低停顿、空闲内存规整. 新生代1. Serial收集器Serial收集器是Hotspot运行在Client模式下的默认新生代收集器, 它的特点是 只用一个CPU/一条收集线程去完成GC工作, 且在进行垃圾收集时必须暂停其他所有的工作线程(“Stop The World” -后面简称STW). 虽然是单线程收集, 但它却简单而高效, 在VM管理内存不大的情况下(收集几十M~一两百M的新生代), 停顿时间完全可以控制在几十毫秒~一百多毫秒内. 2. ParNew收集器ParNew收集器其实是前面Serial的多线程版本, 除使用多条线程进行GC外, 包括Serial可用的所有控制参数、收集算法、STW、对象分配规则、回收策略等都与Serial完全一样(也是VM启用CMS收集器-XX: +UseConcMarkSweepGC的默认新生代收集器). 由于存在线程切换的开销, ParNew在单CPU的环境中比不上Serial, 且在通过超线程技术实现的两个CPU的环境中也不能100%保证能超越Serial. 但随着可用的CPU数量的增加, 收集效率肯定也会大大增加(ParNew收集线程数与CPU的数量相同, 因此在CPU数量过大的环境中, 可用-XX:ParallelGCThreads参数控制GC线程数). 3. Parallel Scavenge收集器与ParNew类似, Parallel Scavenge也是使用复制算法, 也是并行多线程收集器. 但与其他收集器关注尽可能缩短垃圾收集时间不同, Parallel Scavenge更关注系统吞吐量:系统吞吐量=运行用户代码时间(运行用户代码时间+垃圾收集时间)停顿时间越短就越适用于用户交互的程序-良好的响应速度能提升用户的体验;而高吞吐量则适用于后台运算而不需要太多交互的任务-可以最高效率地利用CPU时间,尽快地完成程序的运算任务. Parallel Scavenge提供了如下参数设置系统吞吐量: Parallel Scavenge参数 描述 MaxGCPauseMillis (毫秒数) 收集器将尽力保证内存回收花费的时间不超过设定值, 但如果太小将会导致GC的频率增加. GCTimeRatio (整数:0 &lt; GCTimeRatio &lt; 100) 是垃圾收集时间占总时间的比率 -XX:+UseAdaptiveSizePolicy 启用GC自适应的调节策略: 不再需要手工指定-Xmn、-XX:SurvivorRatio、-XX:PretenureSizeThreshold等细节参数, VM会根据当前系统的运行情况收集性能监控信息, 动态调整这些参数以提供最合适的停顿时间或最大的吞吐量 老年代Serial Old收集器Serial Old是Serial收集器的老年代版本, 同样是单线程收集器,使用“标记-整理”算法: Serial Old应用场景如下: JDK 1.5之前与Parallel Scavenge收集器搭配使用; 作为CMS收集器的后备预案, 在并发收集发生Concurrent Mode Failure时启用(见下:CMS收集器). Parallel Old收集器Parallel Old是Parallel Scavenge收老年代版本, 使用多线程和“标记－整理”算法, 吞吐量优先, 主要与Parallel Scavenge配合在 注重吞吐量 及 CPU资源敏感 系统内使用: CMS收集器CMS(Concurrent Mark Sweep)收集器是一款具有划时代意义的收集器, 一款真正意义上的并发收集器, 虽然现在已经有了理论意义上表现更好的G1收集器, 但现在主流互联网企业线上选用的仍是CMS(如Taobao、微店).CMS是一种以获取最短回收停顿时间为目标的收集器(CMS又称多并发低暂停的收集器), 基于”标记-清除”算法实现, 整个GC过程分为以下4个步骤: 初始标记(CMS initial mark) 并发标记(CMS concurrent mark: GC Roots Tracing过程) 重新标记(CMS remark) 并发清除(CMS concurrent sweep: 已死象将会就地释放, 注意: 此处没有压缩) 其中两个加粗的步骤(初始标记、重新标记)仍需STW. 但初始标记仅只标记一下GC Roots能直接关联到的对象, 速度很快; 而重新标记则是为了修正并发标记期间因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录, 虽然一般比初始标记阶段稍长, 但要远小于并发标记时间. (由于整个GC过程耗时最长的并发标记和并发清除阶段的GC线程可与用户线程一起工作, 所以总体上CMS的GC过程是与用户线程一起并发地执行的.由于CMS收集器将整个GC过程进行了更细粒度的划分, 因此可以实现并发收集、低停顿的优势, 但它也并非十分完美, 其存在缺点及解决策略如下: CMS默认启动的回收线程数=(CPU数目+3)/4当CPU数&gt;4时, GC线程最多占用不超过25%的CPU资源, 但是当CPU数&lt;=4时, GC线程可能就会过多的占用用户CPU资源, 从而导致应用程序变慢, 总吞吐量降低. 无法处理浮动垃圾, 可能出现Promotion Failure、Concurrent Mode Failure而导致另一次Full GC的产生: 浮动垃圾是指在CMS并发清理阶段用户线程运行而产生的新垃圾. 由于在GC阶段用户线程还需运行, 因此还需要预留足够的内存空间给用户线程使用, 导致CMS不能像其他收集器那样等到老年代几乎填满了再进行收集. 因此CMS提供了-XX:CMSInitiatingOccupancyFraction参数来设置GC的触发百分比(以及-XX:+UseCMSInitiatingOccupancyOnly来启用该触发百分比), 当老年代的使用空间超过该比例后CMS就会被触发(JDK 1.6之后默认92%). 但当CMS运行期间预留的内存无法满足程序需要, 就会出现上述Promotion Failure等失败, 这时VM将启动后备预案: 临时启用Serial Old收集器来重新执行Full GC(CMS通常配合大内存使用, 一旦大内存转入串行的Serial GC, 那停顿的时间就是大家都不愿看到的了). 最后, 由于CMS采用”标记-清除”算法实现, 可能会产生大量内存碎片. 内存碎片过多可能会导致无法分配大对象而提前触发Full GC. 因此CMS提供了-XX:+UseCMSCompactAtFullCollection开关参数, 用于在Full GC后再执行一个碎片整理过程. 但内存整理是无法并发的, 内存碎片问题虽然没有了, 但停顿时间也因此变长了, 因此CMS还提供了另外一个参数-XX:CMSFullGCsBeforeCompaction用于设置在执行N次不进行内存整理的Full GC后, 跟着来一次带整理的(默认为0: 每次进入Full GC时都进行碎片整理). 分区收集- G1收集器 G1(Garbage-First)是一款面向服务端应用的收集器, 主要目标用于配备多颗CPU的服务器治理大内存. G1 is planned as the long term replacement for the Concurrent Mark-Sweep Collector (CMS). -XX:+UseG1GC 启用G1收集器. 与其他基于分代的收集器不同, G1将整个Java堆划分为多个大小相等的独立区域(Region), 虽然还保留有新生代和老年代的概念, 但新生代和老年代不再是物理隔离的了, 它们都是一部分Region(不需要连续)的集合. 每块区域既有可能属于O区、也有可能是Y区, 因此不需要一次就对整个老年代/新生代回收. 而是当线程并发寻找可回收的对象时, 有些区块包含可回收的对象要比其他区块多很多. 虽然在清理这些区块时G1仍然需要暂停应用线程, 但可以用相对较少的时间优先回收垃圾较多的Region(这也是G1命名的来源). 这种方式保证了G1可以在有限的时间内获取尽可能高的收集效率. 新生代收集G1的新生代收集跟ParNew类似: 存活的对象被转移到一个/多个Survivor Regions. 如果存活时间达到阀值, 这部分对象就会被提升到老年代. G1的新生代收集特点如下: 一整块堆内存被分为多个Regions. 存活对象被拷贝到新的Survivor区或老年代. 年轻代内存由一组不连续的heap区组成, 这种方法使得可以动态调整各代区域尺寸. Young GCs会有STW事件, 进行时所有应用程序线程都会被暂停. 多线程并发GC. 老年代收集G1老年代GC会执行以下阶段: 注: 一下有些阶段也是年轻代垃圾收集的一部分. index Phase Description (1) 初始标记 (Initial Mark: Stop the World Event) 在G1中, 该操作附着一次年轻代GC, 以标记Survivor中有可能引用到老年代对象的Regions. (2) 扫描根区域 (Root Region Scanning: 与应用程序并发执行) 扫描Survivor中能够引用到老年代的references. 但必须在Minor GC触发前执行完. (3) 并发标记 (Concurrent Marking : 与应用程序并发执行) 在整个堆中查找存活对象, 但该阶段可能会被Minor GC中断. (4) 重新标记 (Remark : Stop the World Event) 完成堆内存中存活对象的标记. 使用snapshot-at-the-beginning(SATB, 起始快照)算法, 比CMS所用算法要快得多(空Region直接被移除并回收, 并计算所有区域的活跃度). (5) 清理 (Cleanup : Stop the World Event and Concurrent) 见下 5-1、2、3 5-1 (Stop the world) 在含有存活对象和完全空闲的区域上进行统计 5-2 (Stop the world) 擦除Remembered Sets. 5-3 (Concurrent) 重置空regions并将他们返还给空闲列表(free list) (*) Copying/Cleanup (Stop the World Event) 选择”活跃度”最低的区域(这些区域可以最快的完成回收). 拷贝/转移存活的对象到新的尚未使用的regions. 该阶段会被记录在gc-log内(只发生年轻代[GC pause (young)], 与老年代一起执行则被记录为[GC Pause (mixed)]. 详细步骤可参考 Oracle官方文档-The G1 Garbage Collector Step by Step. G1老年代GC特点如下: 并发标记阶段(index 3) 在与应用程序并发执行的过程中会计算活跃度信息. 这些活跃度信息标识出那些regions最适合在STW期间回收(which regions will be best to reclaim during an evacuation pause). 不像CMS有清理阶段. 再次标记阶段(index 4) 使用Snapshot-at-the-Beginning(SATB)算法比CMS快得多. 空region直接被回收. 拷贝/清理阶段(Copying/Cleanup Phase) 年轻代与老年代同时回收. 老年代内存回收会基于他的活跃度信息. 补充: 关于Remembered SetG1收集器中, Region之间的对象引用以及其他收集器中的新生代和老年代之间的对象引用都是使用Remembered Set来避免扫描全堆. G1中每个Region都有一个与之对应的Remembered Set, VM发现程序对Reference类型数据进行写操作时, 会产生一个Write Barrier暂时中断写操作, 检查Reference引用的对象是否处于不同的Region中(在分代例子中就是检查是否老年代中的对象引用了新生代的对象), 如果是, 便通过CardTable把相关引用信息记录到被引用对象所属的Region的Remembered Set中. 当内存回收时, 在GC根节点的枚举范围加入Remembered Set即可保证不对全局堆扫描也不会有遗漏. V. JVM小工具在${JAVA_HOME}/bin/目录下Sun/Oracle给我们提供了一些处理应用程序性能问题、定位故障的工具, 包含 bin 描述 功能 jps 打印Hotspot VM进程 VMID、JVM参数、main()函数参数、主类名/Jar路径 jstat 查看Hotspot VM 运行时信息 类加载、内存、GC[可分代查看]、JIT编译 jinfo 查看和修改虚拟机各项配置 -flag name=value jmap heapdump: 生成VM堆转储快照、查询finalize执行队列、Java堆和永久代详细信息 jmap -dump:live,format=b,file=heap.bin [VMID] jstack 查看VM当前时刻的线程快照: 当前VM内每一条线程正在执行的方法堆栈集合 Thread.getAllStackTraces()提供了类似的功能 javap 查看经javac之后产生的JVM字节码代码 自动解析.class文件, 避免了去理解class文件格式以及手动解析class文件内容 jcmd 一个多功能工具, 可以用来导出堆, 查看Java进程、导出线程信息、 执行GC、查看性能相关数据等 几乎集合了jps、jstat、jinfo、jmap、jstack所有功能 jconsole 基于JMX的可视化监视、管理工具 可以查看内存、线程、类、CPU信息, 以及对JMX MBean进行管理 jvisualvm JDK中最强大运行监视和故障处理工具 可以监控内存泄露、跟踪垃圾回收、执行时内存分析、CPU分析、线程分析… VI. VM常用参数整理 参数 描述 -Xms 最小堆大小 -Xmx 最大堆大小 -Xmn 新生代大小 -XX:PermSize 永久代大小 -XX:MaxPermSize 永久代最大大小 -XX:+PrintGC 输出GC日志 -verbose:gc - -XX:+PrintGCDetails 输出GC的详细日志 -XX:+PrintGCTimeStamps 输出GC时间戳(以基准时间的形式) -XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息 -Xloggc:/path/gc.log 日志文件的输出路径 -XX:+PrintGCApplicationStoppedTime 打印由GC产生的停顿时间 在此处无法列举所有的参数以及他们的应用场景, 详细移步Oracle官方文档-Java HotSpot VM Options. 参考 &amp; 扩展深入理解Java虚拟机JVM内幕：Java虚拟机详解 (力荐)JVM中的G1垃圾回收器G1垃圾收集器入门Getting Started with the G1 Garbage Collector深入理解G1垃圾收集器解析JDK 7的Garbage-First收集器The Garbage-First Garbage CollectorMemory Management in the Java HotSpot Virtual MachineJava HotSpot VM OptionsJVM实用参数（一）JVM类型以及编译器模式JVM内存回收理论与实现基于OpenJDK深度定制的淘宝JVM（TaobaoVM） 原文出处： JVM初探- 内存分配、GC原理与垃圾收集器]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统平均负载指标解读]]></title>
    <url>%2F2018%2F08%2F13%2Fa1%2F</url>
    <content type="text"><![CDATA[一.使用top命令,看到右上角有个平均负载指标12345678top - 11:47:03 up 356 days, 20:33, 8 users, load average: 10.08, 11.02, 12.23Tasks: 159 total, 3 running, 156 sleeping, 0 stopped, 0 zombie%Cpu0 : 55.8 us, 11.5 sy, 0.0 ni, 17.3 id, 0.0 wa, 0.0 hi, 15.4 si, 0.0 st%Cpu1 : 36.7 us, 16.3 sy, 0.0 ni, 46.9 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu2 : 37.3 us, 15.7 sy, 0.0 ni, 47.1 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu3 : 30.8 us, 11.5 sy, 0.0 ni, 57.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 16268500 total, 169076 free, 8331188 used, 7768236 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 7399704 avail Mem 当前cpu为4核逻辑处理器,如上所示,怎么判断当前系统是否超载?参考:https://blog.csdn.net/chenxiao_ji/article/details/46897695在Linux shell下，有很多命令可以看到Load Average，例如：12root@Slyar.com:~# uptime12:49:10 up 182 days, 16:54, 2 users, load average: 0.08, 0.04, 0.01 12root@Slyar.com:~# toptop - 12:50:28 up 182 days, 16:55, 2 users, load average: 0.02, 0.05, 0.00 先大致给一下这3个数字的含义：分别表示系统在过去1分钟、5分钟、15分钟内运行进程队列中的平均进程数量。运行队列嘛，没有等待IO，没有WAIT，没有KILL的进程通通都进这个队列。另外还有一个最直接的显示系统平均负载的命令12[172.23.6.189:hadoop@sz-pg-smce-cce-016:/home/hadoop]$ cat /proc/loadavg 12.04 11.32 12.15 26/664 6142 除了前3个数字表示平均进程数量外，后面的1个分数，分母表示系统进程总数，分子表示正在运行的进程数；最后一个数字表示最近运行的进程ID. 二.系统平均负载-进阶解释只是上面那一句话的解释，基本等于没解释。写这篇文章的缘由就是因为看到了一篇老外写的关于Load Average的文章，觉得解释的很好，所以才打算摘取一部分用自己的话翻译一下。@scoutapp Thanks for your article Understanding Linux CPU Load, I just translate and share it to Chinese audiences. 为了更好地理解系统负载，我们用交通流量来做类比。1、单核CPU - 单车道 - 数字在0.00-1.00之间正常路况管理员会告知司机，如果前面比较拥堵，那司机就要等待，如果前面一路畅通，那么司机就可以驾车直接开过。具体来说：0.00-1.00 之间的数字表示此时路况非常良好，没有拥堵，车辆可以毫无阻碍地通过。1.00 表示道路还算正常，但有可能会恶化并造成拥堵。此时系统已经没有多余的资源了，管理员需要进行优化。1.00-*** 表示路况不太好了，如果到达2.00表示有桥上车辆一倍数目的车辆正在等待。这种情况你必须进行检查了。到这里,可以判断,单核CPU处理时,平均负载数超过&gt;=2(一般看近5分钟,中间数值),即CPU出现超载 2、多核CPU - 多车道 - 数字/CPU核数 在0.00-1.00之间正常多核CPU的话，满负荷状态的数字为 “1.00 * CPU核数”，即双核CPU为2.00，四核CPU为4.00。 3、安全的系统平均负载作者认为单核负载在0.7以下是安全的，超过0.7就需要进行优化了。 4、应该看哪一个数字，1分钟，5分钟还是15分钟？作者认为看5分钟和15分钟的比较好，即后面2个数字。 5、怎样知道我的CPU是几核呢？使用以下命令可以直接获得CPU核心数目 grep ‘model name’ /proc/cpuinfo | wc -l 结论:取得CPU核心数目N，观察后面2个数字，用数字/N，如果得到的值小于0.7即可无忧]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven项目引入外部jar]]></title>
    <url>%2F2018%2F07%2F12%2Farticle-5%2F</url>
    <content type="text"><![CDATA[当我们开发一个功能时发现自己的maven仓库中缺少需要的jar怎么办？ 首先将需要的jar下载下来 然后将jar导入到项目中：webapp/WEB-INF/lib目录下 最后在pom.xml文件中加入依赖就可以编译和打包运行了 1234567&lt;dependency&gt; &lt;groupId&gt;fakepath&lt;/groupId&gt; &lt;artifactId&gt;jxl-report&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;basedir&#125;/src/main/webapp/WEB-INF/lib/jxl-report-1.0.jar&lt;/systemPath&gt;&lt;/dependency&gt; systemPath：导入外部jar的路径${basedir}：项目根目录最终路径为：${basedir}/src/main/webapp/WEB-INF/lib/xxx.jar]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>maven</tag>
        <tag>jar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java实现excel模板导出数据]]></title>
    <url>%2F2018%2F07%2F06%2Farticle-4%2F</url>
    <content type="text"><![CDATA[web项目导出excel有很多种方法，个人觉得使用excel模板导出比较好用，可以满足甲方对excel格式的多种需求，而且实现起来方便。 准备需要的jar：下载地址freemarker-2.3.19.jarfreemarker-util-0.0.1.jarjxl-2.6.10.jarjxl-report-1.0.jarmaven项目pom.xml配置：12345678910111213141516171819202122&lt;!-- excel模板依赖start --&gt;&lt;dependency&gt; &lt;groupId&gt;org.freemarker&lt;/groupId&gt; &lt;artifactId&gt;freemarker&lt;/artifactId&gt; &lt;version&gt;2.3.19&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;net.sourceforge.jexcelapi&lt;/groupId&gt; &lt;artifactId&gt;jxl&lt;/artifactId&gt; &lt;version&gt;2.6.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;fakepath&lt;/groupId&gt; &lt;artifactId&gt;freemarker-util&lt;/artifactId&gt; &lt;version&gt;0.0.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;fakepath&lt;/groupId&gt; &lt;artifactId&gt;jxl-report&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- excel模板依赖 end--&gt; java代码实现：1234567891011121314151617181920212223242526272829303132333435@RequestMapping("/exportExcel") @ResponseBody public Map&lt;String,Object&gt; exportExcel(HttpServletRequest request,@RequestBody Map&lt;String, Object&gt; map) throws Exception &#123; logger.info("------------开始执行下载任务-----------"); Map&lt;String,Object&gt; result = new HashMap&lt;String,Object&gt;(); result.put("result",true); result.put("msg","执行成功"); try &#123; String downloadPath="/export";//导出文件夹 //查询导出数据 Map&lt;String,Object&gt; resultMap = reportService.queryExportData(map); //目录生成 ExcelUtil.mkdir(downloadPath); String filename = UUID.randomUUID().toString().replace("-", "").toUpperCase()+".xls"; File f = new File(downloadPath+"/" + File.separatorChar + filename); // 模板生成Excel ReportEnginer enginer = new ReportEnginer(); //模板存储路径 String modelPath = request.getSession().getServletContext().getRealPath("/")+ "/template/model.xls"; InputStream inputStream = new FileInputStream(new File(modelPath)); OutputStream outputStream = new FileOutputStream(f); enginer.excute(inputStream, resultMap, outputStream); inputStream.close(); outputStream.close(); downloadDetail.setResult(filename); &#125; catch (Exception e) &#123; e.printStackTrace(); result.put("result",false); result.put("msg","执行失败"); logger.info("------------下载任务执行失败-----------"); &#125; logger.info("------------下载任务执行完成-----------"); return result; &#125; 代码中resultMap如下：{datalist=[{hours=9, name=张三, cost=10}, {hours=32, name=李四, cost=6}]}excle模板：使用etl表达式结果：]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>excel</tag>
        <tag>模板</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客文字增加背景色块]]></title>
    <url>%2F2018%2F06%2F23%2Farticle-3%2F</url>
    <content type="text"><![CDATA[文字配置效果如下： 站点配置文件 主题配置文件 站点配置文件 主题配置文件 打开themes/next/source/css/_custom 下的 custom.styl 文件,添加属性样式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// 颜色块-黄span#inline-yellow &#123;display:inline;padding:.2em .6em .3em;font-size:80%;font-weight:bold;line-height:1;color:#fff;text-align:center;white-space:nowrap;vertical-align:baseline;border-radius:0;background-color: #f0ad4e;&#125;// 颜色块-绿span#inline-green &#123;display:inline;padding:.2em .6em .3em;font-size:80%;font-weight:bold;line-height:1;color:#fff;text-align:center;white-space:nowrap;vertical-align:baseline;border-radius:0;background-color: #5cb85c;&#125;// 颜色块-蓝span#inline-blue &#123;display:inline;padding:.2em .6em .3em;font-size:80%;font-weight:bold;line-height:1;color:#fff;text-align:center;white-space:nowrap;vertical-align:baseline;border-radius:0;background-color: #2780e3;&#125;// 颜色块-紫span#inline-purple &#123;display:inline;padding:.2em .6em .3em;font-size:80%;font-weight:bold;line-height:1;color:#fff;text-align:center;white-space:nowrap;vertical-align:baseline;border-radius:0;background-color: #9954bb;&#125; 在你需要编辑的文章地方。放置如下代码1234&lt;span id="inline-blue"&gt; 站点配置文件 &lt;/span&gt;&lt;span id="inline-purple"&gt; 主题配置文件 &lt;/span&gt;&lt;span id="inline-yellow"&gt; 站点配置文件 &lt;/span&gt;&lt;span id="inline-green"&gt; 主题配置文件 &lt;/span&gt;]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSL证书让网站从HTTP换成HTTPS]]></title>
    <url>%2F2018%2F06%2F15%2Farticle-2%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HTTP 协议是不加密传输数据的，也就是用户跟你的网站之间传递数据有可能在途中被截获，破解传递的真实内容，所以使用不加密的 HTTP 的网站是不太安全的。所以， Google 的 Chrome 浏览器将在 2017 年 1 月开始，标记使用不加密的 HTTP 协议的网站为 Not Secure，不安全。 如何改变呢，有免费的吗？我的服务器和域名都是阿里云申请的，所以下面方法基于阿里云操作。 证书申请登录阿里云后台，找到，产品与服务-》安全（云盾）-》SSL证书（应用安全），找到购买证书进入购买界面后选择”免费型DV SSL”证书，如下图：订单完成后，在订单页面点击“补全”，补全域名（注意现在免费的证书只能使用填写一个域名，且不支持通配符域名配置，因此不支持域名下的二级域名安全认证），然后填写个人信息，按图填写即可：申请审核通过后会收到邮件，意思是云盾证书开通成功 配置SSL证书SSL证书审核通过后在之前的订单页面就可以看到证书下载入口进入下载页面，点击下载按钮“下载证书for Nginx”，下载证书，然后按照提示操作即可以下是阿里提供的方法：安装证书文件说明：1.证书文件214776764040878.pem，包含两段内容，请不要删除任何一段内容。2.如果是证书系统创建的CSR，还包含：证书私钥文件214776764040878.key。( 1 ) 在Nginx的安装目录下创建cert目录，并且将下载的全部文件拷贝到cert目录中。如果申请证书时是自己创建的CSR文件，请将对应的私钥文件放到cert目录下并且命名为214776764040878.key；( 2 ) 打开 Nginx 安装目录下 conf 目录中的 nginx.conf 文件，找到：12345678910111213141516# HTTPS server# #server &#123;# listen 443;# server_name localhost;# ssl on;# ssl_certificate cert.pem;# ssl_certificate_key cert.key;# ssl_session_timeout 5m;# ssl_protocols SSLv2 SSLv3 TLSv1;# ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP;# ssl_prefer_server_ciphers on;# location / &#123;###&#125;#&#125; ( 3 ) 将其修改为 (以下属性中ssl开头的属性与证书配置有直接关系，其它属性请结合自己的实际情况复制或调整) :1234567891011121314151617server &#123; listen 443; server_name localhost; ssl on; root html; index index.html index.htm; ssl_certificate /usr/local/nginx/cert/214776764040878.pem; ssl_certificate_key /usr/local/nginx/cert/214776764040878.key; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; location / &#123; root html; index index.html index.htm; &#125;&#125; 保存退出。( 4 )重启 Nginx。( 5 ) 通过 https 方式访问您的站点，测试站点证书的安装配置。3.配置443端口SSL使用的443端口，需要服务器开放443端口方法：在阿里云服务器控制台添加安全组，配置443端口监听即可配置后即可通过https访问网站，并且浏览器认证安全参考文档：https://ninghao.net/blog/4449]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>阿里云</tag>
        <tag>SSL证书</tag>
        <tag>HTTPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何运行jar程序]]></title>
    <url>%2F2018%2F06%2F14%2Farticle-1%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在开发过程中我们经常将项目打包成war包，打成war的项目就和可以直接放在tomcat和jetty等中间件中运行。那么jar项目怎么运行呢？最近看到springboot比较流行，好奇下弄了个项目跑了起来，然而springboot是jar项目，直接使用主类的main启动项目，那么打包后的项目怎么运行呢？网上查了查，将启动方法放在这里，以便记忆。 Windows系统运行使用dos命令进入jar所在的目录，直接执行执行命令： java -jar xxx.jar可按CTRL + C打断程序运行，或直接关闭窗口，程序退出 linux系统运行方式一： java -jar XXX.jar特点：当前ssh窗口被锁定，可按CTRL + C打断程序运行，或直接关闭窗口，程序退出方式二： java -jar XXX.jar &amp;特点：当前ssh窗口不被锁定，但是当窗口关闭时，程序中止运行。方式三： nohup java -jar XXX.jar &amp;nohup 意思是不挂断运行命令,当账户退出或终端关闭时,程序仍然运行当用 nohup 命令执行作业时，缺省情况下该作业的所有输出被重定向到nohup.out的文件中，除非另外指定了输出文件。方式四： nohup java -jar XXX.jar &gt;temp.txt &amp;解释下 &gt;temp.txtcommand &gt;out.filecommand &gt;out.file是将command的输出重定向到out.file文件，即输出内容不打印到屏幕上，而是输出到out.file文件中。执行nohup命令会执行失败，这时在命令后面加上2&gt;&amp;1 &amp;即可，执行命名如下： nohup java -jar xxx.jar &gt;logs/log.txt 2&gt;&amp;1 &amp;可通过jobs命令查看后台运行任务 jobs那么就会列出所有后台执行的作业，并且每个作业前面都有个编号。如果想将某个作业调回前台控制，只需要 fg + 编号即可。 fg 23查看某端口占用的线程的pid netstat -nlp |grep :9181]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>jar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有人用古文翻译了当下流行话语，对比一下古文有多美！]]></title>
    <url>%2F2018%2F06%2F12%2F%E6%9C%89%E4%BA%BA%E7%94%A8%E5%8F%A4%E6%96%87%E7%BF%BB%E8%AF%91%E4%BA%86%E5%BD%93%E4%B8%8B%E6%B5%81%E8%A1%8C%E8%AF%9D%E8%AF%AD%EF%BC%8C%E5%AF%B9%E6%AF%94%E4%B8%80%E4%B8%8B%E5%8F%A4%E6%96%87%E6%9C%89%E5%A4%9A%E7%BE%8E%EF%BC%81%2F</url>
    <content type="text"><![CDATA[有人用古文翻译了当下流行话语，对比一下古文有多美！ 【一】 原文：每天都被自己帅到睡不着翻译：玉树临风美少年，揽镜自顾夜不眠。原文：有钱，任性。翻译：家有千金，行止由心。 原文：丑的人都睡了，帅的人还醒着。翻译：玉树立风前，驴骡正酣眠。 原文：主要看气质。翻译：请君莫羡解语花，腹有诗书气自华。 原文：也是醉了。翻译：行迈靡靡，中心如醉。 【二】 原文：人要是没有理想，和咸鱼有什么区别。翻译：涸辙遗鲋，旦暮成枯；人而无志，与彼何殊。 原文：别睡了起来嗨。翻译：昼短苦夜长，何不秉烛游。 原文：不要在意这些细节。翻译：欲图大事，莫拘小节。 原文：你这么牛，家里人知道么。翻译：腰中雄剑长三尺，君家严慈知不知。 原文：心好累。翻译：形若槁骸，心如死灰。 【三】 原文：我的内心几乎是崩溃的。翻译：方寸淆乱，灵台崩摧。 原文：你们城里人真会玩。翻译：城中戏一场，山民笑断肠。 原文：我单方面宣布和xx结婚。翻译：愿出一家之言，以结两姓之好。 原文：重要的事说三遍。翻译：一言难尽意，三令作五申。 原文：世界那么大，我想去看看。翻译：天高地阔，欲往观之。 【四】 原文：明明可以靠脸吃饭，偏偏要靠才华。翻译：中华儿女多奇志，不爱红装爱才智。 原文：我读书少，你不要骗我。翻译：君莫欺我不识字，人间安得有此事。 原文：不作死就不会死，为什么不明白。翻译：幸无白刃驱向前，何用将身自弃捐。 原文：你不是一个人在战斗。翻译：岂曰无衣，与子同袍。 原文：我有知识我自豪。翻译：腹有诗书气自华。 原文：说的好有道理，我竟无言以对。翻译：斯言甚善，余不得赞一词。 【五】 原文：秀恩爱，死的快。翻译：爱而不藏，自取其亡。 原文：吓死宝宝了。翻译：堪惊小儿啼，能开长者颐。 原文：沉默不都是金子，有时候还是孙子。翻译：圣人不言如桃李，小民不言若木鸡。 原文：备胎。翻译：章台之柳，已折他人；玄都之花，未改前度。 原文：屌丝终有逆袭日翻译：王侯将相，宁有种乎？]]></content>
      <categories>
        <category>摘录</category>
      </categories>
      <tags>
        <tag>今文古译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 配置Nginx]]></title>
    <url>%2F2018%2F06%2F11%2Flinux-Nginx%2F</url>
    <content type="text"><![CDATA[Nginx 是 C语言 开发，建议在 Linux 上运行，当然，也可以安装 Windows 版本，本篇则使用 CentOS 7 作为安装环境。 首先下载资源包： 安装Nginx之前，首先要安装好编译环境gcc和g++，然后以CentOS为例安装Nginx，安装Nginx需要PRCE库、zlib库和ssl的支持，除了ssl外其他的我们都是去官网下载： Nginx：http://nginx.org/ PCRE：http://www.pcre.org/ zlib：http://www.zlib.net/ 安装所需环境一. gcc 安装安装 nginx 需要先将官网下载的源码进行编译，编译依赖 gcc 环境，如果没有 gcc 环境，则需要安装： yum install gcc-c++ 二. PCRE pcre-devel 安装PCRE(Perl Compatible Regular Expressions) 是一个Perl库，包括 perl 兼容的正则表达式库。nginx 的 http 模块使用 pcre 来解析正则表达式，所以需要在 linux 上安装 pcre 库，pcre-devel 是使用 pcre 开发的一个二次开发库。nginx也需要此库。命令： yum install -y pcre pcre-devel 三. zlib 安装zlib 库提供了很多种压缩和解压缩的方式， nginx 使用 zlib 对 http 包的内容进行 gzip ，所以需要在 Centos 上安装 zlib 库。 yum install -y zlib zlib-devel 四. OpenSSL 安装OpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。nginx 不仅支持 http 协议，还支持 https（即在ssl协议上传输http），所以需要在 Centos 安装 OpenSSL 库。 yum install -y openssl openssl-devel 安装Nginx：解压Nginx压缩包tar -xvzf nginx-1.9.8.tar.gz，进入解压后文件夹配置：./configure –prefix=/usr/local/nginx –with-http_stub_status_module –with-http_ssl_module –with-http_realip_module 编译1234# make 安装# make install 检查是否安装成功1234cd /usr/local/nginx/sbin ./nginx -t结果显示：nginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is oknginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful 配置防火墙80端口123456#修改防火墙配置： # vi + /etc/sysconfig/iptables #添加配置项 -A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT #重启防火墙 # service iptables restart 启动停止重启与测试1.启动 12345#方法1# /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf#方法2# cd /usr/local/nginx/sbin# ./nginx 2.停止 12345678#查询nginx主进程号 ps -ef | grep nginx#停止进程 kill -QUIT 主进程号 #快速停止 kill -TERM 主进程号 #强制停止 pkill -9 nginx 3.重启(首次启动需：/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf) 1/usr/local/nginx/sbin/nginx -s reload 4.测试 12#测试端口 netstat -na | grep 80]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle 锁表查询和解锁]]></title>
    <url>%2F2018%2F06%2F07%2Foracle-%E9%94%81%E8%A1%A8%E6%9F%A5%E8%AF%A2%E5%92%8C%E8%A7%A3%E9%94%81%2F</url>
    <content type="text"><![CDATA[1.查看里面的锁12345678SELECT b. OWNER, b.object_name, l.session_id, l.locked_modeFROM v$locked_object l, dba_objects bWHERE b.object_id = l.object_id;SELECT t2.username, t2.sid, t2.serial #, t2.logon_timeFROM v$locked_object t1, v$session t2WHERE t1.session_id = t2.sidORDER BY t2.logon_time 2.解锁1alter system kill session 'sid,serial#' 如：1alter system kill session '111,222' 3.查询当前用户的所有活动的session12345select t.SID,t.SERIAL#,t.STATUS,t.STATE,t.SQL_IDfrom v$session twhere t.USERNAME = 'OCN_TDS_DB'and t.STATUS = 'ACTIVE'and t.MACHINE = 'localhost.localdomain'; 4.分析session执行的SQL，尤其是sql_id相同的12#7ykv5kcc4paz2表示当前重复较高的SQL，查询出来发现该SQL主要是用来刷新工单数的。select * from v$sql s where s.SQL_ID='7ykv5kcc4paz2' 5.删除当前应用连接的所有活动session，释放资源1234567#停止Mobile应用，清除所有获取工单数的SQLselect 'alter system kill session '''||t.SID||','||t.SERIAL#||''';'from v$session twhere t.USERNAME = 'OCN_TDS_DB'and t.STATUS = 'ACTIVE'and t.SQL_ID='7ykv5kcc4paz2'and t.MACHINE = 'localhost.localdomain'; 6.根据session_id查询执行的SQL12345678910111213select s.SAMPLE_TIME,sq.SQL_TEXT,sq.DISK_READS,sq.BUFFER_GETS, sq.CPU_TIME,sq.ROWS_PROCESSED,--sq.SQL_FULLTEXT,sq.SQL_IDfrom v$sql sq, v$active_session_history swhere s.SQL_ID = sq.SQL_IDand s.SESSION_ID = 190order by s.SAMPLE_TIME desc;]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo常用命令笔记]]></title>
    <url>%2F2018%2F06%2F06%2Fhexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[hexo npm install hexo -g #安装npm update hexo -g #升级hexo init #初始化 简写 hexo n “我的博客” == hexo new “我的博客” #新建文章 hexo p == hexo publish hexo g == hexo generate#生成 hexo s == hexo server #启动服务预览 hexo d == hexo deploy#部署 服务器 hexo server #Hexo 会监视文件变动并自动更新，您无须重启服务器。hexo server -s #静态模式hexo server -p 5000 #更改端口hexo server -i 192.168.1.1 #自定义 IPhexo clean #清除缓存 网页正常情况下可以忽略此条命令hexo g #生成静态网页hexo d #开始部署 监视文件变动 hexo generate #使用 Hexo 生成静态文件快速而且简单hexo generate –watch #监视文件变动 完成后部署两个命令的作用是相同的 hexo generate –deployhexo deploy –generatehexo deploy -ghexo server -g 草稿 hexo publish [layout] 模版 hexo new “postName” #新建文章hexo new page “pageName” #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，’ctrl + c’关闭server）hexo deploy #将.deploy目录部署到GitHub hexo new [layout] hexo new photo “My Gallery”hexo new “Hello World” –lang tw 变量 描述 layout 布局 title 标题 date 文件建立日期 12345678title: 使用Hexo搭建个人博客&lt;br&gt;layout: post&lt;br&gt;date: 2014-03-03 19:07:43&lt;br&gt;comments: true&lt;br&gt;categories: Blog&lt;br&gt;tags: [Hexo]&lt;br&gt;keywords: Hexo, Blog&lt;br&gt;description: 生命在于折腾，又把博客折腾到Hexo了。给Hexo点赞。 模版（Scaffold） hexo new photo “My Gallery” 变量 描述 layout 布局 title 标题 date 文件建立日期 设置文章摘要以上是文章摘要&lt;!--more--&gt; 以下是余下全文 写作 hexo new page hexo new post 变量 描述 :title 标题 :year 建立的年份（4 位数） :month 建立的月份（2 位数） :i_month 建立的月份（去掉开头的零） :day 建立的日期（2 位数） :i_day 建立的日期（去掉开头的零） 推送到服务器上 hexo n #写文章hexo g #生成hexo d #部署 #可与hexo g合并为 hexo d -g 报错1.找不到git部署1ERROR Deployer not found: git 2.解决方法 npm install hexo-deployer-git –save 3.部署类型设置githexo 3.0 部署类型不再是github，_config.yml 中修改 123456# Deployment## Docs: http://hexo.io/docs/deployment.htmldeploy: type: git repository: git@***.github.com:***/***.github.io.git branch: master 4. xcodebuildxcode-select: error: tool ‘xcodebuild’ requires Xcode, but active developer directory ‘/Library/Developer/CommandLineTools’ is a command line tools instance npm install bcrypt 5. RSS不显示安装RSS插件 npm install hexo-generator-feed –save 开启RSS功能编辑hexo/_config.yml，添加如下代码： 1rss: /atom.xml #rss地址 默认即可 开启评论1.我使用多说代替自带的评论，在多说 网站注册 &gt; 后台管理 &gt; 添加新站点 &gt; 工具 === 复制通用代码 里面有 short_name 1.在根目录 _config.yml 添加一行 disqus_shortname: jslite 是在多说注册时产生的 2.复制到 themes\landscape\layout\_partial\article.ejs 把 1234567&lt;% if (!index &amp;&amp; post.comments &amp;&amp; config.disqus_shortname)&#123; %&gt;&lt;section id="comments"&gt;&lt;div id="disqus_thread"&gt; &lt;noscript&gt;Please enable JavaScript to view the &lt;a href="//disqus.com/?ref_noscript"&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;&lt;/div&gt;&lt;/section&gt;&lt;% &#125; %&gt; 改为 1234567891011121314151617181920&lt;% if (!index &amp;&amp; post.comments &amp;&amp; config.disqus_shortname)&#123; %&gt; &lt;section id="comments"&gt; &lt;!-- 多说评论框 start --&gt; &lt;div class="ds-thread" data-thread-key="&lt;%= post.layout %&gt;-&lt;%= post.slug %&gt;" data-title="&lt;%= post.title %&gt;" data-url="&lt;%= page.permalink %&gt;"&gt;&lt;/div&gt; &lt;!-- 多说评论框 end --&gt; &lt;!-- 多说公共JS代码 start (一个网页只需插入一次) --&gt; &lt;script type="text/javascript"&gt; var duoshuoQuery = &#123;short_name:'&lt;%= config.disqus_shortname %&gt;'&#125;; (function() &#123; var ds = document.createElement('script'); ds.type = 'text/javascript';ds.async = true; ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js'; ds.charset = 'UTF-8'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds); &#125;)(); &lt;/script&gt; &lt;!-- 多说公共JS代码 end --&gt; &lt;/section&gt;&lt;% &#125; %&gt;]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库字段加解密处理]]></title>
    <url>%2F2018%2F06%2F05%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%97%E6%AE%B5%E5%8A%A0%E8%A7%A3%E5%AF%86%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[解决场景： 需要对应用数据存储时采取加密，比如手机号码、地址、证件号方案：其中： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;加解密函数存储在数据库中定义； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;加解密使用的key在应用中定义（定义后不可改变）； 示例：&nbsp;&nbsp;测试用表： 12345CREATE TABLE `sys_user` (`user_id` BIGINT (20) NOT NULL AUTO_INCREMENT,`user_name` VARCHAR (128) DEFAULT NULL,`user_mobile` VARCHAR (128) DEFAULT NULL,PRIMARY KEY (`user_id`)) ENGINE = INNODB DEFAULT CHARSET = utf8 插入：12345INSERT INTO `sys_user` (`user_name`, `user_mobile`) VALUES( 'smartfoot', DATA_ENCRYPT ('13888888888', 'KEY_ABC')); 存储结果:查询：12345SELECT`user_id`,`user_name`,DATA_DECRYPT (`user_mobile`, 'KEY_ABC') user_mobile FROM `sys_user`; 查询结果:模糊匹配：123456SELECT`user_id`,`user_name`,HG_DECRYPT (`user_mobile`, 'KEY_ABC') user_mobile FROM `sys_user` WHERE HG_DECRYPT (`user_mobile`, 'KEY_ABC') LIKE '138%'; 查询结果: 注意事项： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;加解密函数：加解密函数存储在数据库中定义，与应用无关； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;key ：加解密使用的key在应用中定义（定义后不可改变）； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于加密后的长度可变，适当增加加密字段定义长度； Mysql：加密函数:1234567891011121314151617181920DELIMITER $$ DROP FUNCTIONIF EXISTS `DATA_ENCRYPT`$$ CREATE DEFINER = CURRENT_USER FUNCTION `DATA_ENCRYPT` ( p_text VARCHAR (255), p_key VARCHAR (255)) RETURNS VARCHAR (255) CHARSET utf8BEGIN IF (CHAR_LENGTH(p_text) = 0) THEN RETURN '' ;ELSEIF CHAR_LENGTH(p_key) = 0 THEN RETURN p_text ;ELSE RETURN HEX(AES_ENCRYPT(p_text, p_key)) ;ENDIF ; END$$DELIMITER ; 解密函数：12345678910111213141516171819DELIMITER $$DROP FUNCTIONIF EXISTS `DATA_DECRYPT`$$CREATE DEFINER = CURRENT_USER FUNCTION `DATA_DECRYPT` ( p_text VARCHAR (255), p_key VARCHAR (255)) RETURNS VARCHAR (255) CHARSET utf8BEGINIF (CHAR_LENGTH(p_text) = 0) THEN RETURN '' ;ELSEIF CHAR_LENGTH(p_key) = 0 THEN RETURN p_text ;ELSE RETURN AES_DECRYPT(UNHEX(p_text), p_key) ;ENDIF ; END$$DELIMITER ;]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>加密</tag>
      </tags>
  </entry>
</search>
